{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = data.Data()\n",
    "DATA.loadDataFromJSON()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvRNN(object):\n",
    "    \n",
    "    def __init__(self, batch_size, time_steps, output_size, hidden_units, vocabulary_size, layers):\n",
    "        with tf.name_scope(\"Input\"):\n",
    "            self.input = tf.placeholder(tf.int32, shape=(batch_size, time_steps), name=\"input\")\n",
    "            self.labels = tf.placeholder(tf.float32, shape=(batch_size, time_steps, output_size), name=\"targets\")\n",
    "            self.mask = tf.placeholder(tf.float32, shape=(batch_size, time_steps, output_size), name=\"mask\")\n",
    "            self.seq_len = tf.placeholder(tf.int32, [batch_size,])\n",
    "\n",
    "        with tf.name_scope(\"Embedding\"):\n",
    "            embedding_size = hidden_units\n",
    "            self.embedding = tf.get_variable(\"embedding\", (vocabulary_size, embedding_size), \n",
    "                                            initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                            dtype=tf.float32) \n",
    "            #self.embedding = initializer((vocabulary_size, hidden_units))\n",
    "            self.embedded_input = tf.nn.embedding_lookup(self.embedding, self.input, name=\"embedded_input\")\n",
    "\n",
    "        self.input_rnn = self.embedded_input\n",
    "        self.input_rnn = tf.transpose(self.input_rnn)\n",
    "        #self.input_rnn = tf.reshape(self.input_rnn, [batch_size, embedding_size, time_steps//3, 3])\n",
    "        with tf.name_scope(\"RNN_conv\"):\n",
    "            rnn_conv_cell = lambda : tf.contrib.rnn.Conv1DLSTMCell(input_shape=[embedding_size, time_steps],\n",
    "                                                                   output_channels=3,\n",
    "                                                                   use_bias=True,\n",
    "                                                                   kernel_shape=3)\n",
    "            cells_fw = [\n",
    "                    rnn_conv_cell()\n",
    "                    for _ in range(layers)]\n",
    "    \n",
    "            cells_bw = [\n",
    "                    rnn_conv_cell()\n",
    "                    for _ in range(layers)]\n",
    "                \n",
    "            self.state = ([cell.zero_state(batch_size, dtype=tf.float32) for cell in cells_fw],\n",
    "                              [cell.zero_state(batch_size, dtype=tf.float32) for cell in cells_bw])\n",
    "            outputs, states_fw, states_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "                                                cells_fw = cells_fw,\n",
    "                                                cells_bw=cells_bw,\n",
    "                                                initial_states_fw = self.state[0],\n",
    "                                                initial_states_bw = self.state[1],\n",
    "                                                sequence_length = self.seq_len,\n",
    "                                                dtype = tf.float32,\n",
    "                                                inputs = self.input_rnn)\n",
    "                \n",
    "            self.next_state = (states_fw, states_bw)\n",
    "            #self.outputs_rnn = tf.concat(outputs, 2)\n",
    "            self.outputs_rnn = outputs\n",
    "            hidden_layer_size = hidden_units*2\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yolkin/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1711: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Conv Linear expects 3D, 4D or 5D arguments: [[300, 32], [32, 300, 3]]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-aafe9f727cc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConvRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDATA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetPartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs_rnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-a2606e89cbab>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch_size, time_steps, output_size, hidden_units, vocabulary_size, layers)\u001b[0m\n\u001b[1;32m     41\u001b[0m                                                 \u001b[0msequence_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                                                 \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                                                 inputs = self.input_rnn)\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstates_fw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_bw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn.py\u001b[0m in \u001b[0;36mstack_bidirectional_dynamic_rnn\u001b[0;34m(cells_fw, cells_bw, inputs, initial_states_fw, initial_states_bw, dtype, sequence_length, parallel_iterations, time_major, scope)\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             time_major=time_major)\n\u001b[0m\u001b[1;32m    234\u001b[0m         \u001b[0;31m# Concat the outputs to create the new input.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0mprev_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mbidirectional_dynamic_rnn\u001b[0;34m(cell_fw, cell_bw, inputs, sequence_length, initial_state_fw, initial_state_bw, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    410\u001b[0m           \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_state_fw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m           \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m           time_major=time_major, scope=fw_scope)\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;31m# Backward direction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36mdynamic_rnn\u001b[0;34m(cell, inputs, sequence_length, initial_state, dtype, parallel_iterations, swap_memory, time_major, scope)\u001b[0m\n\u001b[1;32m    625\u001b[0m         \u001b[0mswap_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mswap_memory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m         \u001b[0msequence_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         dtype=dtype)\n\u001b[0m\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;31m# Outputs of _dynamic_rnn_loop are always shaped [time, batch, depth].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_dynamic_rnn_loop\u001b[0;34m(cell, inputs, initial_state, parallel_iterations, swap_memory, sequence_length, dtype)\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0mparallel_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparallel_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m       \u001b[0mmaximum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 824\u001b[0;31m       swap_memory=swap_memory)\n\u001b[0m\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m   \u001b[0;31m# Unpack final output if not using output tuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations)\u001b[0m\n\u001b[1;32m   3222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3223\u001b[0m       \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_to_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWHILE_CONTEXT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3224\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloop_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBuildLoop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape_invariants\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmaximum_iterations\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mBuildLoop\u001b[0;34m(self, pred, body, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2954\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2955\u001b[0m         original_body_result, exit_vars = self._BuildLoop(\n\u001b[0;32m-> 2956\u001b[0;31m             pred, body, original_loop_vars, loop_vars, shape_invariants)\n\u001b[0m\u001b[1;32m   2957\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2958\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m_BuildLoop\u001b[0;34m(self, pred, body, original_loop_vars, loop_vars, shape_invariants)\u001b[0m\n\u001b[1;32m   2891\u001b[0m         flat_sequence=vars_for_body_with_tensor_arrays)\n\u001b[1;32m   2892\u001b[0m     \u001b[0mpre_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2893\u001b[0;31m     \u001b[0mbody_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpacked_vars_for_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2894\u001b[0m     \u001b[0mpost_summaries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SUMMARY_COLLECTION\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2895\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   3192\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   3193\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m-> 3194\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_time_step\u001b[0;34m(time, output_ta_t, state)\u001b[0m\n\u001b[1;32m    791\u001b[0m           \u001b[0mcall_cell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcall_cell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m           \u001b[0mstate_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m           skip_conditionals=True)\n\u001b[0m\u001b[1;32m    794\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m_rnn_step\u001b[0;34m(time, sequence_length, min_sequence_length, max_sequence_length, zero_output, state, call_cell, state_size, skip_conditionals)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0;31m# steps.  This is faster when max_seq_len is equal to the number of unrolls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;31m# (which is typical for dynamic_rnn).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0mnew_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_same_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \u001b[0minput_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflat_sequence\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 781\u001b[0;31m     \u001b[0mcall_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msequence_length\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope_attrname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNNCell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_rnn_get_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    718\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             raise ValueError('A layer\\'s `call` method should return a Tensor '\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, state, scope)\u001b[0m\n\u001b[1;32m   2111\u001b[0m     \u001b[0mcell\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2112\u001b[0m     new_hidden = _conv([inputs, hidden], self._kernel_shape,\n\u001b[0;32m-> 2113\u001b[0;31m                        4 * self._output_channels, self._use_bias)\n\u001b[0m\u001b[1;32m   2114\u001b[0m     gates = array_ops.split(\n\u001b[1;32m   2115\u001b[0m         value=new_hidden, num_or_size_splits=4, axis=self._conv_ndims + 1)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/rnn/python/ops/rnn_cell.py\u001b[0m in \u001b[0;36m_conv\u001b[0;34m(args, filter_size, num_features, bias, bias_start)\u001b[0m\n\u001b[1;32m   2184\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2185\u001b[0m       raise ValueError(\"Conv Linear expects 3D, 4D \"\n\u001b[0;32m-> 2186\u001b[0;31m                        \"or 5D arguments: %s\" % str(shapes))\n\u001b[0m\u001b[1;32m   2187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2188\u001b[0m       raise ValueError(\"Conv Linear expects all args \"\n",
      "\u001b[0;31mValueError\u001b[0m: Conv Linear expects 3D, 4D or 5D arguments: [[300, 32], [32, 300, 3]]"
     ]
    }
   ],
   "source": [
    "session=tf.InteractiveSession()\n",
    "tf.reset_default_graph()\n",
    "model = ConvRNN(32, 60, 2, 300, DATA.getPartition(1).vocabulary_size, 2)\n",
    "print(tf.shape(model.outputs_rnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_size = np.max(Y_labels[:,1])+1\n",
    "output_size = 2\n",
    "learning_rate = 1e-1\n",
    "batch_size = 32\n",
    "hidden_size = 600\n",
    "'''\n",
    "num_features depends how do u we want to represent data\n",
    "we can use w2v embbeding to send dense represetation\n",
    "#num_features =embeddings.shape[1]\n",
    "we can use sparse representation which for this example requires over 5000 long one hot vector\n",
    "we can use sparse representation taking only instruction without arguments lowering one hot to 203 dim\n",
    "\n",
    "in this simple model dense representation didnt show better result then filtered representation \n",
    "with only instructions\n",
    "'''\n",
    "\n",
    "#num_features = len(ByteCode.mnemonicMap)\n",
    "num_features =embeddings.shape[0] + 1\n",
    "num_epochs = 15\n",
    "\n",
    "sequence_length = 100\n",
    "\n",
    "rnn2_graph = tf.Graph()\n",
    "\n",
    "with rnn2_graph.as_default():\n",
    "    \n",
    "    sequence = tf.placeholder(tf.int32,[batch_size, sequence_length])\n",
    "    labels= tf.placeholder(tf.float64,[batch_size, sequence_length, output_size])\n",
    "    seq_len = tf.placeholder(tf.int64, [batch_size])\n",
    "    mask = tf.placeholder(tf.float64, [batch_size, sequence_length, output_size])\n",
    "\n",
    "    cells_fw = [\n",
    "        tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.contrib.rnn.BasicLSTMCell(hidden_size,activation=tf.nn.tanh),\n",
    "        output_keep_prob = 0.8)\n",
    "        for _ in range(1)]\n",
    "    \n",
    "    cells_bw = [\n",
    "        tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.contrib.rnn.BasicLSTMCell(hidden_size,activation=tf.nn.tanh),\n",
    "        output_keep_prob = 0.8)\n",
    "        for _ in range(1)]\n",
    "    \n",
    "    W_embed = tf.get_variable(\n",
    "         'W', shape=(num_features, hidden_size), initializer=tf.contrib.layers.xavier_initializer(), dtype=tf.float64)\n",
    "    \n",
    "    \n",
    "    W_1 = tf.Variable(tf.random_normal([1,hidden_size*2, hidden_size*2], dtype=tf.float64))\n",
    "    b_1 = tf.Variable(tf.random_normal([hidden_size*2],dtype=tf.float64))\n",
    "    W_2 = tf.Variable(tf.random_normal([1,hidden_size*2, output_size], dtype=tf.float64))\n",
    "    b_2 = tf.Variable(tf.random_normal([output_size], dtype=tf.float64))\n",
    "\n",
    "    #initial_state = cell_fw.zero_state(batch_size, dtype=tf.float64)\n",
    "\n",
    "\n",
    "    #outputs, state = tf.nn.dynamic_rnn(cell, sequence, \n",
    "    #        initial_state=initial_state, sequence_length=seq_len)\n",
    "\n",
    "    embed_input = tf.nn.embedding_lookup(W_embed, sequence, name = \"embedded_input\")\n",
    "    \n",
    "    outputs, states_fw, states_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "        cells_fw = cells_fw,\n",
    "        cells_bw=cells_bw,\n",
    "        dtype = tf.float64,\n",
    "        sequence_length = seq_len,\n",
    "        inputs = embed_input)\n",
    "    \n",
    "    outputs = tf.concat(outputs, 2)\n",
    "    \n",
    "    outputs_2 = tf.nn.relu(tf.matmul(outputs, tf.tile(W_1, [tf.shape(outputs)[0],1,1])) + b_1)\n",
    "    logits  = tf.matmul(outputs_2, tf.tile(W_2, [tf.shape(outputs_2)[0],1,1])) + b_2\n",
    "    logits = logits*mask\n",
    "    '''\n",
    "    for output_batch, label_batch in zip(tf.unstack(outputs, axis=1), tf.unstack(labels, axis=1)):\n",
    "        for output, label in zip(tf.unstack(output_batch, axis=0), tf.unstack(label_batch, axis=0)):\n",
    "            output = tf.expand_dims(output, 0)\n",
    "            label = tf.expand_dims(label, 0)\n",
    "            logits = tf.matmul(output, W)+ b\n",
    "            loss += tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits)\n",
    "            correct_predictions += tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1)), tf.float64))\n",
    "        #incorrect_prediciton += batch_size - tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1)),tf.float64))\n",
    "    '''\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits = logits)\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    #accuracy = correct_predictions/tf.reduce_sum(tf.cast(seq_len,tf.float64))\n",
    "    #acc_val, acc_op = tf.metrics.accuracy(tf.argmax(labels,axis=2), tf.argmax(logits, axis=2))\n",
    "    diff = batch_size*sequence_length - tf.reduce_sum(tf.cast(tf.equal(tf.argmax(labels, axis =2), tf.argmax(logits, axis=2)), tf.float32))\n",
    "    nonpadsum = tf.cast(tf.reduce_sum(seq_len),tf.float32)\n",
    "    accuracy = (nonpadsum- diff)/nonpadsum\n",
    "    #correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4993\n",
      "555\n",
      "5548\n",
      "BASELINE: 0.9123273159846137\n",
      "Epoch 0, step 16 ,acc: 0.888588\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 1, step 16 ,acc: 0.914673\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 2, step 16 ,acc: 0.915384\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 3, step 16 ,acc: 0.922356\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 4, step 16 ,acc: 0.922094\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 5, step 16 ,acc: 0.927536\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 6, step 16 ,acc: 0.928848\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 7, step 16 ,acc: 0.929959\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 8, step 16 ,acc: 0.932536\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 9, step 16 ,acc: 0.933738\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Epoch 10, step 16 ,acc: 0.932848\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 11, step 16 ,acc: 0.934635\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 12, step 16 ,acc: 0.935145\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 13, step 16 ,acc: 0.936062\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 14, step 16 ,acc: 0.935148\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "N = int(0.9*X_train.shape[0])\n",
    "testN = X_train.shape[0]-N\n",
    "\n",
    "assert N+testN == X_train.shape[0]\n",
    "n_steps = N//batch_size\n",
    "\n",
    "new_Y = []\n",
    "ln, acsum = 0,0\n",
    "for y_seq in Y_train:\n",
    "    y_seq = list(map(lambda x: ((x[0],1) if x[1]==1 else (x[0],0)), y_seq))\n",
    "    new_Y.append(y_seq)\n",
    "    acsum += sum(list(map((lambda x:x[1]), y_seq)))\n",
    "    ln += len(y_seq)\n",
    "Y_train = new_Y\n",
    "print('BASELINE:', 1-acsum/ln)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "from random import shuffle\n",
    "indices = np.arange(X_train.shape[0])\n",
    "shuffle(indices)\n",
    "\n",
    "X_train, X_test = X_train[indices[:N]], X_train[indices[N:N+testN]]\n",
    "Y_train, Y_test = Y_train[indices[:N]], Y_train[indices[N:N+testN]]\n",
    "\n",
    "n_steps_test = int(testN/batch_size)\n",
    "\n",
    "with tf.Session(graph=rnn2_graph) as session:\n",
    "    iop = tf.global_variables_initializer()\n",
    "    loc = tf.local_variables_initializer()\n",
    "    session.run(iop)\n",
    "    #session.run(loc)\n",
    "    for epoch in range(num_epochs):\n",
    "        acc_sum=0\n",
    "        batch_index = 0\n",
    "        for step in range(n_steps):\n",
    "            cur_batch_size = 0\n",
    "            batch = []\n",
    "            batch_Y = []\n",
    "            sequence_len = []\n",
    "            while(cur_batch_size < batch_size):\n",
    "                cur_seq = []\n",
    "                y_cur_seq = []\n",
    "                for x,y in zip(X_train[batch_index],Y_train[batch_index]):\n",
    "                    if x in ByteCode.mnemonicMap:\n",
    "                        cur_seq.append(x)\n",
    "                        y_cur_seq.append(y)\n",
    "                    if len(cur_seq) == sequence_length:\n",
    "                        batch.append(cur_seq)\n",
    "                        batch_Y.append(y_cur_seq)\n",
    "                        sequence_len.append(len(y_cur_seq))\n",
    "                        cur_batch_size += 1\n",
    "                        cur_seq = []\n",
    "                        y_cur_seq = []\n",
    "                if cur_seq:\n",
    "                    batch.append(cur_seq)\n",
    "                    batch_Y.append(y_cur_seq)\n",
    "                    sequence_len.append(len(y_cur_seq))\n",
    "                    cur_batch_size += 1\n",
    "                    batch_index += 1\n",
    "            \n",
    "            data = np.zeros([batch_size, sequence_length])\n",
    "            filled_labels = np.zeros([batch_size, sequence_length, output_size])\n",
    "            filled_labels[:,:,0] = 0\n",
    "            mask_val = np.zeros([batch_size, sequence_length, output_size])\n",
    "            for i, seq in enumerate(batch_Y):\n",
    "                for j, label in enumerate(seq):\n",
    "                    onehot=np.zeros(output_size)\n",
    "                    onehot[label[1]] = 1\n",
    "                    filled_labels[i,j,:] = onehot\n",
    "                    mask_val[i,j,:] = 1\n",
    "            for i in range(batch_size):\n",
    "                for j in range(sequence_len[i]):\n",
    "                    data[i,j] = dictionary.get(batch[i][j],0)+1\n",
    "            \n",
    "            feed = {sequence:data, labels:filled_labels, seq_len:sequence_len, mask:mask_val}\n",
    "            ops = [logits, diff, accuracy, train_step]\n",
    "            logits_val, diff_val, acc_fixed, _= session.run(ops, feed_dict=feed)\n",
    "            #acc_fixed = session.run([accuracy], feed_dict={seq_len:sequence_len})\n",
    "            #session.run(loc)\n",
    "            \n",
    "            #print(acc_fixed)\n",
    "            #print(diff_val)\n",
    "            #print(np.sum(filled_labels))\n",
    "            #print(np.sum(sequence_len))\n",
    "            #print(np.argmax(filled_labels,axis=2)[0][:sequence_len[0]])\n",
    "            #print(np.argmax(logits_val,axis=2)[0][:sequence_len[0]])\n",
    "            '''\n",
    "            zum = 0\n",
    "            for i,sl in enumerate(sequence_len):\n",
    "                zum += np.sum(np.argmax(logits_val[i,sl:,:],axis=1))\n",
    "            print(zum)\n",
    "            '''\n",
    "            #print()\n",
    "\n",
    "        acc_sum=0\n",
    "        batch_index = 0\n",
    "        for step in range(n_steps_test):\n",
    "            cur_batch_size = 0\n",
    "            batch = []\n",
    "            batch_Y = []\n",
    "            sequence_len = []\n",
    "            while(cur_batch_size < batch_size):\n",
    "                cur_seq = []\n",
    "                y_cur_seq = []\n",
    "                for x,y in zip(X_test[batch_index],Y_test[batch_index]):\n",
    "                    if x in ByteCode.mnemonicMap:\n",
    "                        cur_seq.append(x)\n",
    "                        y_cur_seq.append(y)\n",
    "                    if len(cur_seq) == sequence_length:\n",
    "                        batch.append(cur_seq)\n",
    "                        batch_Y.append(y_cur_seq)\n",
    "                        sequence_len.append(len(y_cur_seq))\n",
    "                        cur_batch_size += 1\n",
    "                        cur_seq = []\n",
    "                        y_cur_seq = []\n",
    "                if cur_seq:\n",
    "                    batch.append(cur_seq)\n",
    "                    batch_Y.append(y_cur_seq)\n",
    "                    sequence_len.append(len(y_cur_seq))\n",
    "                    cur_batch_size += 1\n",
    "                    batch_index += 1\n",
    "            \n",
    "            data = np.zeros([batch_size, sequence_length])\n",
    "            filled_labels = np.zeros([batch_size, sequence_length, output_size])\n",
    "            filled_labels[:,:,0] = 0\n",
    "            mask_val = np.zeros([batch_size, sequence_length, output_size])\n",
    "            for i, seq in enumerate(batch_Y):\n",
    "                for j, label in enumerate(seq):\n",
    "                    onehot=np.zeros(output_size)\n",
    "                    onehot[label[1]] = 1\n",
    "                    filled_labels[i,j,:] = onehot\n",
    "                    mask_val[i,j,:] = 1\n",
    "            for i in range(batch_size):\n",
    "                for j in range(sequence_len[i]):\n",
    "                    data[i,j] = dictionary.get(batch[i][j],0)+1\n",
    "            \n",
    "            feed = {sequence:data, labels:filled_labels, seq_len:sequence_len, mask:mask_val}\n",
    "            ops = [logits, diff, accuracy]\n",
    "            logits_val, diff_val, acc_fixed = session.run(ops, feed_dict=feed)\n",
    "            acc_sum += acc_fixed\n",
    "            \n",
    "        print(\"Epoch %d, step %d ,acc: %f\" % (epoch, step, acc_sum/n_steps_test))\n",
    "        index = np.argmax(np.sum(filled_labels,axis=(1,2)))\n",
    "        print(batch[index])\n",
    "        print(np.argmax(filled_labels,axis=2)[index][:sequence_len[index]])\n",
    "        print(np.argmax(logits_val,axis=2)[index][:sequence_len[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
