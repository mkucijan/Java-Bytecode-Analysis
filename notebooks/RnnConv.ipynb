{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.loadtxt(os.path.join(dir_path, 'cache', 'embeddings.vec'))\n",
    "\n",
    "pickle_in = open('cache/database.dict', 'rb')\n",
    "db = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open('cache/data2onehot.dict', 'rb')\n",
    "dictionary = pickle.load(pickle_in)\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate data by length and by each label in one class 'Y_labels'\n",
    "# and by jump instruction 'Y_if'\n",
    "#\n",
    "from JavaClassParser import ByteCode\n",
    "reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
    "X_train=[]\n",
    "Y_train=[]\n",
    "X_train_long=[]\n",
    "Y_train_long=[]\n",
    "X_labels = []\n",
    "Y_labels = []\n",
    "\n",
    "X_if = []\n",
    "Y_if = []\n",
    "\n",
    "for dclass in db.values():\n",
    "    for method in dclass.values():\n",
    "        instructions = method['x']\n",
    "        labels = method['y']\n",
    "        byteIndex = method['index']\n",
    "        \n",
    "        \n",
    "        #\n",
    "        #seperating by labels\n",
    "\n",
    "        cur_section = []\n",
    "        cur_label = labels[0]\n",
    "        if len(instructions)<100:\n",
    "            X_train.append(instructions)\n",
    "            Y_train.append(labels)\n",
    "        else:\n",
    "            X_train_long.append(instructions)\n",
    "            Y_train_long.append(labels)\n",
    "        for instruction, label in zip(instructions,labels):\n",
    "            if label != cur_label:\n",
    "                X_labels.append(cur_section)\n",
    "                Y_labels.append(cur_label)\n",
    "                cur_section = []\n",
    "                cur_label = label\n",
    "            cur_section.append(dictionary.get(instruction,0))\n",
    "        \n",
    "        #print(instructions)\n",
    "        \n",
    "        \n",
    "        #\n",
    "        #seperating by if jumps\n",
    "        \n",
    "        for i,label in zip(range(len(instructions)),labels):\n",
    "        #index = method[i]\n",
    "        #instruction = reverse_dictionary[index]\n",
    "            instruction = instructions[i]\n",
    "            if 'if' in instruction[:2]:\n",
    "                #offset = int(reverse_dictionary[method[i+1]])\n",
    "                offset = int(instructions[i+1])\n",
    "                j = i\n",
    "                step = np.sign(offset)\n",
    "                curr = byteIndex[i]\n",
    "                end = curr+offset\n",
    "                section = []\n",
    "                while(curr!=end):\n",
    "                    section.append(dictionary.get(instructions[j],0))\n",
    "                    if instructions[j] in ByteCode.mnemonicMap:\n",
    "                        #curr += ByteCode.mnemonicMap[instructions[j]].getOpCodeCount() + 1\n",
    "                        curr = byteIndex[j]\n",
    "                    j += step\n",
    "                    if curr>=abs(offset):\n",
    "                        argCounts = ByteCode.mnemonicMap[instructions[j-step]].argsCount\n",
    "                        for arg in argCounts:\n",
    "                            if arg > 0:\n",
    "                                section.append(dictionary.get(instructions[j],0))\n",
    "                                j += step\n",
    "                        break\n",
    "\n",
    "                if offset<0:\n",
    "                    section = list(reversed(section))\n",
    "                    section.append(dictionary.get(instructions[i+1],0))\n",
    "                \n",
    "                X_if.append(section)\n",
    "                Y_if.append(label)\n",
    "                \n",
    "        #'''\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_train_long = np.array(X_train_long)\n",
    "Y_train_long = np.array(Y_train_long)\n",
    "X_labels = np.array(X_labels)\n",
    "Y_labels = np.array(Y_labels)\n",
    "X_if = np.array(X_if)\n",
    "Y_if = np.array(Y_if)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_size = np.max(Y_labels[:,1])+1\n",
    "output_size = 2\n",
    "learning_rate = 1e-1\n",
    "batch_size = 32\n",
    "hidden_size = 600\n",
    "'''\n",
    "num_features depends how do u we want to represent data\n",
    "we can use w2v embbeding to send dense represetation\n",
    "#num_features =embeddings.shape[1]\n",
    "we can use sparse representation which for this example requires over 5000 long one hot vector\n",
    "we can use sparse representation taking only instruction without arguments lowering one hot to 203 dim\n",
    "\n",
    "in this simple model dense representation didnt show better result then filtered representation \n",
    "with only instructions\n",
    "'''\n",
    "\n",
    "#num_features = len(ByteCode.mnemonicMap)\n",
    "num_features =embeddings.shape[0] + 1\n",
    "num_epochs = 15\n",
    "\n",
    "sequence_length = 100\n",
    "\n",
    "rnn2_graph = tf.Graph()\n",
    "\n",
    "with rnn2_graph.as_default():\n",
    "    \n",
    "    sequence = tf.placeholder(tf.int32,[batch_size, sequence_length])\n",
    "    labels= tf.placeholder(tf.float64,[batch_size, sequence_length, output_size])\n",
    "    seq_len = tf.placeholder(tf.int64, [batch_size])\n",
    "    mask = tf.placeholder(tf.float64, [batch_size, sequence_length, output_size])\n",
    "\n",
    "    cells_fw = [\n",
    "        tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.contrib.rnn.BasicLSTMCell(hidden_size,activation=tf.nn.tanh),\n",
    "        output_keep_prob = 0.8)\n",
    "        for _ in range(1)]\n",
    "    \n",
    "    cells_bw = [\n",
    "        tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.contrib.rnn.BasicLSTMCell(hidden_size,activation=tf.nn.tanh),\n",
    "        output_keep_prob = 0.8)\n",
    "        for _ in range(1)]\n",
    "    \n",
    "    W_embed = tf.get_variable(\n",
    "         'W', shape=(num_features, hidden_size), initializer=tf.contrib.layers.xavier_initializer(), dtype=tf.float64)\n",
    "    \n",
    "    \n",
    "    W_1 = tf.Variable(tf.random_normal([1,hidden_size*2, hidden_size*2], dtype=tf.float64))\n",
    "    b_1 = tf.Variable(tf.random_normal([hidden_size*2],dtype=tf.float64))\n",
    "    W_2 = tf.Variable(tf.random_normal([1,hidden_size*2, output_size], dtype=tf.float64))\n",
    "    b_2 = tf.Variable(tf.random_normal([output_size], dtype=tf.float64))\n",
    "\n",
    "    #initial_state = cell_fw.zero_state(batch_size, dtype=tf.float64)\n",
    "\n",
    "\n",
    "    #outputs, state = tf.nn.dynamic_rnn(cell, sequence, \n",
    "    #        initial_state=initial_state, sequence_length=seq_len)\n",
    "\n",
    "    embed_input = tf.nn.embedding_lookup(W_embed, sequence, name = \"embedded_input\")\n",
    "    \n",
    "    outputs, states_fw, states_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "        cells_fw = cells_fw,\n",
    "        cells_bw=cells_bw,\n",
    "        dtype = tf.float64,\n",
    "        sequence_length = seq_len,\n",
    "        inputs = embed_input)\n",
    "    \n",
    "    outputs = tf.concat(outputs, 2)\n",
    "    \n",
    "    outputs_2 = tf.nn.relu(tf.matmul(outputs, tf.tile(W_1, [tf.shape(outputs)[0],1,1])) + b_1)\n",
    "    logits  = tf.matmul(outputs_2, tf.tile(W_2, [tf.shape(outputs_2)[0],1,1])) + b_2\n",
    "    logits = logits*mask\n",
    "    '''\n",
    "    for output_batch, label_batch in zip(tf.unstack(outputs, axis=1), tf.unstack(labels, axis=1)):\n",
    "        for output, label in zip(tf.unstack(output_batch, axis=0), tf.unstack(label_batch, axis=0)):\n",
    "            output = tf.expand_dims(output, 0)\n",
    "            label = tf.expand_dims(label, 0)\n",
    "            logits = tf.matmul(output, W)+ b\n",
    "            loss += tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits)\n",
    "            correct_predictions += tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1)), tf.float64))\n",
    "        #incorrect_prediciton += batch_size - tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1)),tf.float64))\n",
    "    '''\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits = logits)\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    #accuracy = correct_predictions/tf.reduce_sum(tf.cast(seq_len,tf.float64))\n",
    "    #acc_val, acc_op = tf.metrics.accuracy(tf.argmax(labels,axis=2), tf.argmax(logits, axis=2))\n",
    "    diff = batch_size*sequence_length - tf.reduce_sum(tf.cast(tf.equal(tf.argmax(labels, axis =2), tf.argmax(logits, axis=2)), tf.float32))\n",
    "    nonpadsum = tf.cast(tf.reduce_sum(seq_len),tf.float32)\n",
    "    accuracy = (nonpadsum- diff)/nonpadsum\n",
    "    #correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4993\n",
      "555\n",
      "5548\n",
      "BASELINE: 0.9123273159846137\n",
      "Epoch 0, step 16 ,acc: 0.888588\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 1, step 16 ,acc: 0.914673\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 2, step 16 ,acc: 0.915384\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 3, step 16 ,acc: 0.922356\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 4, step 16 ,acc: 0.922094\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 5, step 16 ,acc: 0.927536\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 6, step 16 ,acc: 0.928848\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 7, step 16 ,acc: 0.929959\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 8, step 16 ,acc: 0.932536\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 9, step 16 ,acc: 0.933738\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      "Epoch 10, step 16 ,acc: 0.932848\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 1 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 11, step 16 ,acc: 0.934635\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[1 0 1 0 1 0 0 0 1 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 12, step 16 ,acc: 0.935145\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 1 0 0 0 0 0 1 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 13, step 16 ,acc: 0.936062\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 1 0 1 0 1 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Epoch 14, step 16 ,acc: 0.935148\n",
      "['lload_0', 'lload_2', 'ladd', 'lstore', 'lload_0', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'iflt', 'lload', 'lload_2', 'lxor', 'lconst_0', 'lcmp', 'ifge', 'new', 'dup', 'getstatic', 'iconst_2', 'anewarray', 'dup', 'iconst_0', 'lload_0', 'invokestatic', 'aastore', 'dup', 'iconst_1', 'lload_2', 'invokestatic', 'aastore', 'invokespecial', 'athrow', 'lload', 'lreturn']\n",
      "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 1 1 0 1 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "N = int(0.9*X_train.shape[0])\n",
    "testN = X_train.shape[0]-N\n",
    "\n",
    "assert N+testN == X_train.shape[0]\n",
    "n_steps = N//batch_size\n",
    "\n",
    "new_Y = []\n",
    "ln, acsum = 0,0\n",
    "for y_seq in Y_train:\n",
    "    y_seq = list(map(lambda x: ((x[0],1) if x[1]==1 else (x[0],0)), y_seq))\n",
    "    new_Y.append(y_seq)\n",
    "    acsum += sum(list(map((lambda x:x[1]), y_seq)))\n",
    "    ln += len(y_seq)\n",
    "Y_train = new_Y\n",
    "print('BASELINE:', 1-acsum/ln)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "from random import shuffle\n",
    "indices = np.arange(X_train.shape[0])\n",
    "shuffle(indices)\n",
    "\n",
    "X_train, X_test = X_train[indices[:N]], X_train[indices[N:N+testN]]\n",
    "Y_train, Y_test = Y_train[indices[:N]], Y_train[indices[N:N+testN]]\n",
    "\n",
    "n_steps_test = int(testN/batch_size)\n",
    "\n",
    "with tf.Session(graph=rnn2_graph) as session:\n",
    "    iop = tf.global_variables_initializer()\n",
    "    loc = tf.local_variables_initializer()\n",
    "    session.run(iop)\n",
    "    #session.run(loc)\n",
    "    for epoch in range(num_epochs):\n",
    "        acc_sum=0\n",
    "        batch_index = 0\n",
    "        for step in range(n_steps):\n",
    "            cur_batch_size = 0\n",
    "            batch = []\n",
    "            batch_Y = []\n",
    "            sequence_len = []\n",
    "            while(cur_batch_size < batch_size):\n",
    "                cur_seq = []\n",
    "                y_cur_seq = []\n",
    "                for x,y in zip(X_train[batch_index],Y_train[batch_index]):\n",
    "                    if x in ByteCode.mnemonicMap:\n",
    "                        cur_seq.append(x)\n",
    "                        y_cur_seq.append(y)\n",
    "                    if len(cur_seq) == sequence_length:\n",
    "                        batch.append(cur_seq)\n",
    "                        batch_Y.append(y_cur_seq)\n",
    "                        sequence_len.append(len(y_cur_seq))\n",
    "                        cur_batch_size += 1\n",
    "                        cur_seq = []\n",
    "                        y_cur_seq = []\n",
    "                if cur_seq:\n",
    "                    batch.append(cur_seq)\n",
    "                    batch_Y.append(y_cur_seq)\n",
    "                    sequence_len.append(len(y_cur_seq))\n",
    "                    cur_batch_size += 1\n",
    "                    batch_index += 1\n",
    "            \n",
    "            data = np.zeros([batch_size, sequence_length])\n",
    "            filled_labels = np.zeros([batch_size, sequence_length, output_size])\n",
    "            filled_labels[:,:,0] = 0\n",
    "            mask_val = np.zeros([batch_size, sequence_length, output_size])\n",
    "            for i, seq in enumerate(batch_Y):\n",
    "                for j, label in enumerate(seq):\n",
    "                    onehot=np.zeros(output_size)\n",
    "                    onehot[label[1]] = 1\n",
    "                    filled_labels[i,j,:] = onehot\n",
    "                    mask_val[i,j,:] = 1\n",
    "            for i in range(batch_size):\n",
    "                for j in range(sequence_len[i]):\n",
    "                    data[i,j] = dictionary.get(batch[i][j],0)+1\n",
    "            \n",
    "            feed = {sequence:data, labels:filled_labels, seq_len:sequence_len, mask:mask_val}\n",
    "            ops = [logits, diff, accuracy, train_step]\n",
    "            logits_val, diff_val, acc_fixed, _= session.run(ops, feed_dict=feed)\n",
    "            #acc_fixed = session.run([accuracy], feed_dict={seq_len:sequence_len})\n",
    "            #session.run(loc)\n",
    "            \n",
    "            #print(acc_fixed)\n",
    "            #print(diff_val)\n",
    "            #print(np.sum(filled_labels))\n",
    "            #print(np.sum(sequence_len))\n",
    "            #print(np.argmax(filled_labels,axis=2)[0][:sequence_len[0]])\n",
    "            #print(np.argmax(logits_val,axis=2)[0][:sequence_len[0]])\n",
    "            '''\n",
    "            zum = 0\n",
    "            for i,sl in enumerate(sequence_len):\n",
    "                zum += np.sum(np.argmax(logits_val[i,sl:,:],axis=1))\n",
    "            print(zum)\n",
    "            '''\n",
    "            #print()\n",
    "\n",
    "        acc_sum=0\n",
    "        batch_index = 0\n",
    "        for step in range(n_steps_test):\n",
    "            cur_batch_size = 0\n",
    "            batch = []\n",
    "            batch_Y = []\n",
    "            sequence_len = []\n",
    "            while(cur_batch_size < batch_size):\n",
    "                cur_seq = []\n",
    "                y_cur_seq = []\n",
    "                for x,y in zip(X_test[batch_index],Y_test[batch_index]):\n",
    "                    if x in ByteCode.mnemonicMap:\n",
    "                        cur_seq.append(x)\n",
    "                        y_cur_seq.append(y)\n",
    "                    if len(cur_seq) == sequence_length:\n",
    "                        batch.append(cur_seq)\n",
    "                        batch_Y.append(y_cur_seq)\n",
    "                        sequence_len.append(len(y_cur_seq))\n",
    "                        cur_batch_size += 1\n",
    "                        cur_seq = []\n",
    "                        y_cur_seq = []\n",
    "                if cur_seq:\n",
    "                    batch.append(cur_seq)\n",
    "                    batch_Y.append(y_cur_seq)\n",
    "                    sequence_len.append(len(y_cur_seq))\n",
    "                    cur_batch_size += 1\n",
    "                    batch_index += 1\n",
    "            \n",
    "            data = np.zeros([batch_size, sequence_length])\n",
    "            filled_labels = np.zeros([batch_size, sequence_length, output_size])\n",
    "            filled_labels[:,:,0] = 0\n",
    "            mask_val = np.zeros([batch_size, sequence_length, output_size])\n",
    "            for i, seq in enumerate(batch_Y):\n",
    "                for j, label in enumerate(seq):\n",
    "                    onehot=np.zeros(output_size)\n",
    "                    onehot[label[1]] = 1\n",
    "                    filled_labels[i,j,:] = onehot\n",
    "                    mask_val[i,j,:] = 1\n",
    "            for i in range(batch_size):\n",
    "                for j in range(sequence_len[i]):\n",
    "                    data[i,j] = dictionary.get(batch[i][j],0)+1\n",
    "            \n",
    "            feed = {sequence:data, labels:filled_labels, seq_len:sequence_len, mask:mask_val}\n",
    "            ops = [logits, diff, accuracy]\n",
    "            logits_val, diff_val, acc_fixed = session.run(ops, feed_dict=feed)\n",
    "            acc_sum += acc_fixed\n",
    "            \n",
    "        print(\"Epoch %d, step %d ,acc: %f\" % (epoch, step, acc_sum/n_steps_test))\n",
    "        index = np.argmax(np.sum(filled_labels,axis=(1,2)))\n",
    "        print(batch[index])\n",
    "        print(np.argmax(filled_labels,axis=2)[index][:sequence_len[index]])\n",
    "        print(np.argmax(logits_val,axis=2)[index][:sequence_len[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
