{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yolkin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.loadtxt(os.path.join(dir_path, 'cache', 'embeddings.vec'))\n",
    "\n",
    "pickle_in = open(os.path.join(dir_path,'cache', 'database.dict'), 'rb')\n",
    "db = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open(os.path.join(dir_path,'cache','data2onehot.dict'), 'rb')\n",
    "dictionary = pickle.load(pickle_in)\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate data by length and by each label in one class 'Y_labels'\n",
    "# and by jump instruction 'Y_if'\n",
    "#\n",
    "from JavaClassParser import ByteCode\n",
    "reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
    "X_train=[]\n",
    "Y_train=[]\n",
    "X_train_long=[]\n",
    "Y_train_long=[]\n",
    "X_labels = []\n",
    "Y_labels = []\n",
    "\n",
    "X_if = []\n",
    "Y_if = []\n",
    "\n",
    "for dclass in db.values():\n",
    "    for method in dclass.values():\n",
    "        instructions = method['x']\n",
    "        labels = method['y']\n",
    "        byteIndex = method['index']\n",
    "        \n",
    "        \n",
    "        #\n",
    "        #seperating by labels\n",
    "\n",
    "        cur_section = []\n",
    "        cur_label = labels[0]\n",
    "        if len(instructions)<100:\n",
    "            X_train.append(instructions)\n",
    "            Y_train.append(labels)\n",
    "        else:\n",
    "            X_train_long.append(instructions)\n",
    "            Y_train_long.append(labels)\n",
    "        for instruction, label in zip(instructions,labels):\n",
    "            if label != cur_label:\n",
    "                X_labels.append(cur_section)\n",
    "                Y_labels.append(cur_label)\n",
    "                cur_section = []\n",
    "                cur_label = label\n",
    "            cur_section.append(dictionary.get(instruction,0))\n",
    "        \n",
    "        #print(instructions)\n",
    "        \n",
    "        \n",
    "        #\n",
    "        #seperating by if jumps\n",
    "        \n",
    "        for i,label in zip(range(len(instructions)),labels):\n",
    "        #index = method[i]\n",
    "        #instruction = reverse_dictionary[index]\n",
    "            instruction = instructions[i]\n",
    "            if 'if' in instruction[:2]:\n",
    "                #offset = int(reverse_dictionary[method[i+1]])\n",
    "                offset = int(instructions[i+1])\n",
    "                j = i\n",
    "                step = np.sign(offset)\n",
    "                curr = byteIndex[i]\n",
    "                end = curr+offset\n",
    "                section = []\n",
    "                while(curr!=end):\n",
    "                    section.append(dictionary.get(instructions[j],0))\n",
    "                    if instructions[j] in ByteCode.mnemonicMap:\n",
    "                        #curr += ByteCode.mnemonicMap[instructions[j]].getOpCodeCount() + 1\n",
    "                        curr = byteIndex[j]\n",
    "                    j += step\n",
    "                    if curr>=abs(offset):\n",
    "                        argCounts = ByteCode.mnemonicMap[instructions[j-step]].argsCount\n",
    "                        for arg in argCounts:\n",
    "                            if arg > 0:\n",
    "                                section.append(dictionary.get(instructions[j],0))\n",
    "                                j += step\n",
    "                        break\n",
    "\n",
    "                if offset<0:\n",
    "                    section = list(reversed(section))\n",
    "                    section.append(dictionary.get(instructions[i+1],0))\n",
    "                \n",
    "                X_if.append(section)\n",
    "                Y_if.append(label)\n",
    "                \n",
    "        #'''\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_train_long = np.array(X_train_long)\n",
    "Y_train_long = np.array(Y_train_long)\n",
    "X_labels = np.array(X_labels)\n",
    "Y_labels = np.array(Y_labels)\n",
    "X_if = np.array(X_if)\n",
    "Y_if = np.array(Y_if)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-d96f24976e4c>:83: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#output_size = np.max(Y_labels[:,1])+1\n",
    "output_size = 2\n",
    "learning_rate = 1e-1\n",
    "batch_size = 64\n",
    "hidden_size = 8\n",
    "'''\n",
    "num_features depends how do u we want to represent data\n",
    "we can use w2v embbeding to send dense represetation\n",
    "#num_features =embeddings.shape[1]\n",
    "we can use sparse representation which for this example requires over 5000 long one hot vector\n",
    "we can use sparse representation taking only instruction without arguments lowering one hot to 203 dim\n",
    "\n",
    "in this simple model dense representation didnt show better result then filtered representation \n",
    "with only instructions\n",
    "'''\n",
    "\n",
    "#num_features = len(ByteCode.mnemonicMap)\n",
    "num_features =embeddings.shape[0] + 1\n",
    "num_epochs = 15\n",
    "\n",
    "sequence_length = 100\n",
    "\n",
    "rnn2_graph = tf.Graph()\n",
    "\n",
    "with rnn2_graph.as_default():\n",
    "    \n",
    "    sequence = tf.placeholder(tf.int32,[batch_size, sequence_length])\n",
    "    labels= tf.placeholder(tf.float64,[batch_size, sequence_length, output_size])\n",
    "    seq_len = tf.placeholder(tf.int64, [batch_size])\n",
    "    mask = tf.placeholder(tf.float64, [batch_size, sequence_length, output_size])\n",
    "\n",
    "    cells_fw = [\n",
    "        tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.contrib.rnn.BasicLSTMCell(hidden_size,activation=tf.nn.tanh),\n",
    "        output_keep_prob = 0.8)\n",
    "        for _ in range(1)]\n",
    "    \n",
    "    cells_bw = [\n",
    "        tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.contrib.rnn.BasicLSTMCell(hidden_size,activation=tf.nn.tanh),\n",
    "        output_keep_prob = 0.8)\n",
    "        for _ in range(1)]\n",
    "    \n",
    "    W_embed = tf.get_variable(\n",
    "         'W', shape=(num_features, hidden_size), initializer=tf.contrib.layers.xavier_initializer(), dtype=tf.float64)\n",
    "    \n",
    "    \n",
    "    W_1 = tf.Variable(tf.random_normal([1,hidden_size*2, hidden_size*2], dtype=tf.float64))\n",
    "    b_1 = tf.Variable(tf.random_normal([hidden_size*2],dtype=tf.float64))\n",
    "    W_2 = tf.Variable(tf.random_normal([1,hidden_size*2, output_size], dtype=tf.float64))\n",
    "    b_2 = tf.Variable(tf.random_normal([output_size], dtype=tf.float64))\n",
    "\n",
    "    #initial_state = cell_fw.zero_state(batch_size, dtype=tf.float64)\n",
    "\n",
    "\n",
    "    #outputs, state = tf.nn.dynamic_rnn(cell, sequence, \n",
    "    #        initial_state=initial_state, sequence_length=seq_len)\n",
    "\n",
    "    embed_input = tf.nn.embedding_lookup(W_embed, sequence, name = \"embedded_input\")\n",
    "    \n",
    "    outputs, states_fw, states_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "        cells_fw = cells_fw,\n",
    "        cells_bw=cells_bw,\n",
    "        dtype = tf.float64,\n",
    "        sequence_length = seq_len,\n",
    "        inputs = embed_input)\n",
    "    \n",
    "    outputs = tf.concat(outputs, 2)\n",
    "    \n",
    "    outputs_2 = tf.nn.relu(tf.matmul(outputs, tf.tile(W_1, [tf.shape(outputs)[0],1,1])) + b_1)\n",
    "    logits  = tf.matmul(outputs_2, tf.tile(W_2, [tf.shape(outputs_2)[0],1,1])) + b_2\n",
    "    logits = logits*mask\n",
    "    '''\n",
    "    for output_batch, label_batch in zip(tf.unstack(outputs, axis=1), tf.unstack(labels, axis=1)):\n",
    "        for output, label in zip(tf.unstack(output_batch, axis=0), tf.unstack(label_batch, axis=0)):\n",
    "            output = tf.expand_dims(output, 0)\n",
    "            label = tf.expand_dims(label, 0)\n",
    "            logits = tf.matmul(output, W)+ b\n",
    "            loss += tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits)\n",
    "            correct_predictions += tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1)), tf.float64))\n",
    "        #incorrect_prediciton += batch_size - tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1)),tf.float64))\n",
    "    '''\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits = logits)\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    #accuracy = correct_predictions/tf.reduce_sum(tf.cast(seq_len,tf.float64))\n",
    "    #acc_val, acc_op = tf.metrics.accuracy(tf.argmax(labels,axis=2), tf.argmax(logits, axis=2))\n",
    "    diff = batch_size*sequence_length - tf.reduce_sum(tf.cast(tf.equal(tf.argmax(labels, axis =2), tf.argmax(logits, axis=2)), tf.float32))\n",
    "    nonpadsum = tf.cast(tf.reduce_sum(seq_len),tf.float32)\n",
    "    accuracy = (nonpadsum- diff)/nonpadsum\n",
    "    #correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model import data\n",
    "db = data.Data()\n",
    "db.loadDataFromJSON()\n",
    "db.relabelData(overwrite=True)\n",
    "part_1 = db.getPartition(0.9)\n",
    "part_2 = db.getPartition(0.1)\n",
    "dictionary = part_1.getVocabulary()\n",
    "#X_train, Y_train = data.Data.flattenData(part.X, part.Y, sequence_length)\n",
    "#X_train, Y_train = part.X, part.Y\n",
    "#X_train = np.array(X_train)\n",
    "#Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13337\n",
      "0.9253762289643525\n",
      "0.8913384234211458\n",
      "TRAIN: 0.8486078977584839\n",
      "TRAIN: 0.8689414476553291\n",
      "TRAIN: 0.7169536046220709\n",
      "TRAIN: 0.7571340513116793\n",
      "TRAIN: 0.7880443758691338\n",
      "TRAIN: 0.8071195566557837\n",
      "TRAIN: 0.8175480761063787\n",
      "TRAIN: 0.8264226770965902\n",
      "TRAIN: 0.8336423437972706\n",
      "TRAIN: 0.8351591821541094\n",
      "TRAIN: 0.8450936465960196\n",
      "TRAIN: 0.8474495293290623\n",
      "TRAIN: 0.8478598118718758\n",
      "TRAIN: 0.8457758789641446\n",
      "TRAIN: 0.8479592877716542\n",
      "TRAIN: 0.8481793803919153\n",
      "TRAIN: 0.8524568073017036\n",
      "TRAIN: 0.8520558507979656\n",
      "TRAIN: 0.8510714444669394\n",
      "TRAIN: 0.8528617526216702\n",
      "TRAIN: 0.8522682402724914\n",
      "TRAIN: 0.8524363254705405\n",
      "TRAIN: 0.8515791587277389\n",
      "TRAIN: 0.8529150935381499\n",
      "TRAIN: 0.8542596557037173\n",
      "TRAIN: 0.8555005598121532\n",
      "TRAIN: 0.8571784577852275\n",
      "TRAIN: 0.8582703570754919\n",
      "TRAIN: 0.859865740448901\n",
      "TRAIN: 0.8611519256935658\n",
      "TRAIN: 0.8620214668329046\n",
      "TRAIN: 0.8617361571624161\n",
      "TRAIN: 0.8626780469456924\n",
      "TRAIN: 0.8634815859806482\n",
      "TRAIN: 0.8640257774330883\n",
      "TRAIN: 0.8647426713148645\n",
      "TRAIN: 0.8655755561367491\n",
      "TRAIN: 0.8669800341337266\n",
      "TRAIN: 0.8658226072786847\n",
      "TRAIN: 0.8665960337402528\n",
      "TRAIN: 0.8679527580417837\n",
      "TRAIN: 0.86914150041885\n",
      "TRAIN: 0.8701784686649782\n",
      "TRAIN: 0.8709360468598176\n",
      "TRAIN: 0.8712190162786553\n",
      "TRAIN: 0.8720331950875239\n",
      "TRAIN: 0.8722705861163962\n",
      "TRAIN: 0.8733254090672273\n",
      "TRAIN: 0.8725777570437396\n",
      "TRAIN: 0.8743861406312078\n",
      "TRAIN: 0.8751436887365053\n",
      "TRAIN: 0.8760938173889476\n",
      "TRAIN: 0.876752629305395\n",
      "TRAIN: 0.8773778914124748\n",
      "TRAIN: 0.8777937659698487\n",
      "TRAIN: 0.8784661605323244\n",
      "TRAIN: 0.8789557797736034\n",
      "TRAIN: 0.8792590138382415\n",
      "TRAIN: 0.880511968932711\n",
      "TRAIN: 0.881514740327543\n",
      "TRAIN: 0.8821201337720975\n",
      "TRAIN: 0.8831584793976188\n",
      "TRAIN: 0.8843208705219434\n",
      "TRAIN: 0.8850884418921594\n",
      "TRAIN: 0.8857780639510573\n",
      "TRAIN: 0.8863600978450403\n",
      "TRAIN: 0.8869949783961959\n",
      "TRAIN: 0.8865731761853196\n",
      "TRAIN: 0.8868559999726336\n",
      "TRAIN: 0.8873705702723681\n",
      "TRAIN: 0.8881223062744897\n",
      "TRAIN: 0.8887072019574496\n",
      "TRAIN: 0.8891779693401942\n",
      "TRAIN: 0.8893953934968009\n",
      "TRAIN: 0.8898701226823722\n",
      "TRAIN: 0.8902567514210489\n",
      "TRAIN: 0.8908057029601346\n",
      "TRAIN: 0.891583801635846\n",
      "TRAIN: 0.8920724468832203\n",
      "TRAIN: 0.8951104717054216\n",
      "TRAIN: 0.9001940634034247\n",
      "TRAIN: 0.9011439005980117\n",
      "TRAIN: 0.905191368857377\n",
      "TRAIN: 0.9097250208750463\n",
      "TRAIN: 0.9135603937057176\n",
      "TRAIN: 0.9173449509428563\n",
      "TRAIN: 0.9181329373015333\n",
      "TRAIN: 0.9185185915400023\n",
      "TRAIN: 0.9188049654977751\n",
      "TRAIN: 0.9192531592239438\n",
      "TRAIN: 0.9197201045001128\n",
      "TRAIN: 0.9200847462481923\n",
      "TRAIN: 0.9205837251766965\n",
      "TRAIN: 0.9209695280774124\n",
      "TRAIN: 0.9221281574190178\n",
      "TRAIN: 0.9225343696767326\n",
      "TRAIN: 0.9230055114544814\n",
      "TRAIN: 0.9237660001381387\n",
      "TRAIN: 0.9237280519153923\n",
      "TRAIN: 0.923855113984313\n",
      "TRAIN: 0.9238665466729683\n",
      "TRAIN: 0.9238300785978721\n",
      "TRAIN: 0.9238241283294695\n",
      "TRAIN: 0.9240561030389306\n",
      "TRAIN: 0.9242714299902102\n",
      "TRAIN: 0.9244765021786113\n",
      "TRAIN: 0.9247108341127176\n",
      "TRAIN: 0.9249175493052862\n",
      "TRAIN: 0.9251701715210977\n",
      "TRAIN: 0.9249987061258986\n",
      "TRAIN: 0.9249879173798804\n",
      "TRAIN: 0.9252411829550262\n",
      "TRAIN: 0.9253839292154129\n",
      "TRAIN: 0.925637206382119\n",
      "TRAIN: 0.925965070196092\n",
      "TRAIN: 0.9262030472727649\n",
      "TRAIN: 0.9279801057965061\n",
      "TRAIN: 0.9281589742904706\n",
      "TRAIN: 0.9284507542216638\n",
      "TRAIN: 0.928799165161561\n",
      "TRAIN: 0.929073085688838\n",
      "TRAIN: 0.9291965266096529\n",
      "TRAIN: 0.9291482891342868\n",
      "TRAIN: 0.9296234026376591\n",
      "TRAIN: 0.9299140979027897\n",
      "TRAIN: 0.9303589101730483\n",
      "TRAIN: 0.9310034309435948\n",
      "TRAIN: 0.9317383377516459\n",
      "TRAIN: 0.9326808461605448\n",
      "TRAIN: 0.9336765143999617\n",
      "TRAIN: 0.9345204838925927\n",
      "TRAIN: 0.9347519020629819\n",
      "TRAIN: 0.9350603109541253\n",
      "TRAIN: 0.9360764832479134\n",
      "TRAIN: 0.9362649754404382\n",
      "TRAIN: 0.9365784100019795\n",
      "TRAIN: 0.9360079674282521\n",
      "TRAIN: 0.9368067641202895\n",
      "TRAIN: 0.9367225089197403\n",
      "TRAIN: 0.9363406939965258\n",
      "TRAIN: 0.9363137284198962\n",
      "TRAIN: 0.9365356076914422\n",
      "TRAIN: 0.9365866676774774\n",
      "TRAIN: 0.9365079376761815\n",
      "TRAIN: 0.936640873102576\n",
      "TRAIN: 0.936544249185618\n",
      "TRAIN: 0.9366548124739433\n",
      "TRAIN: 0.9366495028630363\n",
      "TRAIN: 0.936497706576463\n",
      "TRAIN: 0.9367608996731944\n",
      "TRAIN: 0.936938136675552\n",
      "TRAIN: 0.9364742337767807\n",
      "TRAIN: 0.9360470812972183\n",
      "TRAIN: 0.9364623477062275\n",
      "TRAIN: 0.9373570493827856\n",
      "TRAIN: 0.938775963180917\n",
      "TRAIN: 0.9401208764764121\n",
      "TRAIN: 0.9413510763834277\n",
      "TRAIN: 0.942242512784348\n",
      "TRAIN: 0.9434496002980495\n",
      "TRAIN: 0.944588877494851\n",
      "TRAIN: 0.9456273045954923\n",
      "TRAIN: 0.9459246014408234\n",
      "TRAIN: 0.9461676912768404\n",
      "TRAIN: 0.9462549801820749\n",
      "TRAIN: 0.9463036195131196\n",
      "TRAIN: 0.9464729896608687\n",
      "TRAIN: 0.9464669475345685\n",
      "TRAIN: 0.9465602900053299\n",
      "TRAIN: 0.946541726947321\n",
      "TRAIN: 0.9467476876261113\n",
      "TRAIN: 0.9468713772682924\n",
      "TRAIN: 0.9471426185764295\n",
      "TRAIN: 0.9472619605029634\n",
      "TRAIN: 0.9474770084680244\n",
      "TRAIN: 0.9475536834791631\n",
      "TRAIN: 0.9476610698100237\n",
      "TRAIN: 0.9479088667561287\n",
      "TRAIN: 0.9480221535930256\n",
      "TRAIN: 0.9481026447680934\n",
      "TRAIN: 0.9482705763415237\n",
      "TRAIN: 0.948115398828949\n",
      "TRAIN: 0.9485268877922413\n",
      "TRAIN: 0.9485738210656732\n",
      "TRAIN: 0.9486386233741213\n",
      "TRAIN: 0.948666043166145\n",
      "TRAIN: 0.9487746626726672\n",
      "TRAIN: 0.9489484623525528\n",
      "TRAIN: 0.9489930944986965\n",
      "TRAIN: 0.9491020525384042\n",
      "TRAIN: 0.9492124883698748\n",
      "TRAIN: 0.9492748440475326\n",
      "TRAIN: 0.949270397444927\n",
      "TRAIN: 0.949590697137342\n",
      "TRAIN: 0.9502747853715799\n",
      "TRAIN: 0.9509413248033107\n",
      "TRAIN: 0.9515899316184635\n",
      "TRAIN: 0.9522217577101132\n",
      "TRAIN: 0.952897263356014\n",
      "TRAIN: 0.9535816506193522\n",
      "TRAIN: 0.9542464348646419\n",
      "TRAIN: 0.9542207987323087\n",
      "TRAIN: 0.9541170448643407\n",
      "TRAIN: 0.9541214893011659\n",
      "TRAIN: 0.9540376191932911\n",
      "TRAIN: 0.9541165440269225\n",
      "TRAIN: 0.9540956261628094\n",
      "TRAIN: 0.953871846653321\n",
      "Epoch 0, step 21 ,acc: 0.943727\n",
      "0.9437272337690165\n",
      "['dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'dmul', 'dload_3', 'dload', 'dmul', 'dsub', 'dstore', 'dload_3', 'dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'iflt', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'ifge', 'aload_1', 'arraylength', 'iconst_1', 'isub', 'istore', 'aload_1', 'iload', 'aaload', 'invokevirtual', 'aload_1', 'iconst_0', 'aaload', 'invokevirtual', 'dsub', 'dstore', 'dload', 'dconst_0', 'dcmpl', 'ifne', 'new', 'dup', 'invokespecial', 'athrow', 'aload_2', 'iconst_1', 'ldc2_w', 'dload', 'ddiv', 'dastore', 'ldc2_w', 'dstore', 'ldc2_w', 'dstore']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "TRAIN: 0.9437354803085327\n",
      "TRAIN: 0.9352462118149913\n",
      "TRAIN: 0.9252990941142841\n",
      "TRAIN: 0.9365633374072032\n",
      "TRAIN: 0.9380692144383025\n",
      "TRAIN: 0.9439795737553278\n",
      "TRAIN: 0.9459134575839226\n",
      "TRAIN: 0.9453141524621743\n",
      "TRAIN: 0.9474127745519857\n",
      "TRAIN: 0.9491213855695377\n",
      "TRAIN: 0.9431548391694099\n",
      "TRAIN: 0.9414645790726235\n",
      "TRAIN: 0.9412812401802504\n",
      "TRAIN: 0.9385533907425198\n",
      "TRAIN: 0.9398656021301095\n",
      "TRAIN: 0.9388572266920675\n",
      "TRAIN: 0.9400647972355737\n",
      "TRAIN: 0.9399257100242793\n",
      "TRAIN: 0.9427503710337549\n",
      "TRAIN: 0.9444152767369757\n",
      "TRAIN: 0.9440034520548568\n",
      "TRAIN: 0.9427948542463191\n",
      "TRAIN: 0.94039845859199\n",
      "TRAIN: 0.9401215348382325\n",
      "TRAIN: 0.9406171207675821\n",
      "TRAIN: 0.9409867519409573\n",
      "TRAIN: 0.9412827499613964\n",
      "TRAIN: 0.9414005405267293\n",
      "TRAIN: 0.941710625244856\n",
      "TRAIN: 0.941933592245784\n",
      "TRAIN: 0.9417149789508638\n",
      "TRAIN: 0.9390855981548601\n",
      "TRAIN: 0.9392013822799385\n",
      "TRAIN: 0.9397034984710574\n",
      "TRAIN: 0.939493137689648\n",
      "TRAIN: 0.9397935392244996\n",
      "TRAIN: 0.9397330869612863\n",
      "TRAIN: 0.9398866108690705\n",
      "TRAIN: 0.9402228544412179\n",
      "TRAIN: 0.9403920164378342\n",
      "TRAIN: 0.9407657645798988\n",
      "TRAIN: 0.9410565498268554\n",
      "TRAIN: 0.9412116418457724\n",
      "TRAIN: 0.9411995928957331\n",
      "TRAIN: 0.9398006089650699\n",
      "TRAIN: 0.9391908754204319\n",
      "TRAIN: 0.9387959015692903\n",
      "TRAIN: 0.938999580451773\n",
      "TRAIN: 0.9369676017022021\n",
      "TRAIN: 0.9375507550651939\n",
      "TRAIN: 0.9377346954756162\n",
      "TRAIN: 0.9378390156984604\n",
      "TRAIN: 0.9376594385169824\n",
      "TRAIN: 0.9374036017823123\n",
      "TRAIN: 0.9372666197868967\n",
      "TRAIN: 0.9374156546505192\n",
      "TRAIN: 0.9370804506875846\n",
      "TRAIN: 0.9359298048758777\n",
      "TRAIN: 0.935935391262101\n",
      "TRAIN: 0.93639444592483\n",
      "TRAIN: 0.9356912677322256\n",
      "TRAIN: 0.93595777680158\n",
      "TRAIN: 0.9364134048082897\n",
      "TRAIN: 0.9365212739199817\n",
      "TRAIN: 0.936726773052978\n",
      "TRAIN: 0.9367352692267864\n",
      "TRAIN: 0.9365642457132768\n",
      "TRAIN: 0.934044461984448\n",
      "TRAIN: 0.9334490354329761\n",
      "TRAIN: 0.9335715365849693\n",
      "TRAIN: 0.9339726380198861\n",
      "TRAIN: 0.9341624028824597\n",
      "TRAIN: 0.9342318554741321\n",
      "TRAIN: 0.9341622482889781\n",
      "TRAIN: 0.934277560880348\n",
      "TRAIN: 0.934287143880381\n",
      "TRAIN: 0.9342911331921401\n",
      "TRAIN: 0.9346238115131138\n",
      "TRAIN: 0.9347873064261988\n",
      "TRAIN: 0.9365145797596397\n",
      "TRAIN: 0.939591473712025\n",
      "TRAIN: 0.9399360032818406\n",
      "TRAIN: 0.9423952053110326\n",
      "TRAIN: 0.9451498078247657\n",
      "TRAIN: 0.9474801427510668\n",
      "TRAIN: 0.9497796026209817\n",
      "TRAIN: 0.949952854534583\n",
      "TRAIN: 0.9500227599337612\n",
      "TRAIN: 0.9500699474832242\n",
      "TRAIN: 0.9502524570604968\n",
      "TRAIN: 0.9501815921485155\n",
      "TRAIN: 0.950376649941765\n",
      "TRAIN: 0.9504949593624894\n",
      "TRAIN: 0.9505617180074671\n",
      "TRAIN: 0.9509191147524515\n",
      "TRAIN: 0.9497139458875776\n",
      "TRAIN: 0.9497168531426016\n",
      "TRAIN: 0.9501266719867881\n",
      "TRAIN: 0.949776322146529\n",
      "TRAIN: 0.9494662345154781\n",
      "TRAIN: 0.9489072792450477\n",
      "TRAIN: 0.9490613665909451\n",
      "TRAIN: 0.9489799914996626\n",
      "TRAIN: 0.9489052807374655\n",
      "TRAIN: 0.9489808441879763\n",
      "TRAIN: 0.948488091002675\n",
      "TRAIN: 0.9484323885991974\n",
      "TRAIN: 0.9485526699158171\n",
      "TRAIN: 0.9486539613231773\n",
      "TRAIN: 0.9486194123162635\n",
      "TRAIN: 0.9485564479462023\n",
      "TRAIN: 0.9486764126881665\n",
      "TRAIN: 0.9485233599260111\n",
      "TRAIN: 0.9485034679512974\n",
      "TRAIN: 0.9486525225333827\n",
      "TRAIN: 0.9488298652829358\n",
      "TRAIN: 0.9500517010552633\n",
      "TRAIN: 0.94995540372523\n",
      "TRAIN: 0.9500079840554093\n",
      "TRAIN: 0.9501977825648483\n",
      "TRAIN: 0.9502082058447588\n",
      "TRAIN: 0.9501296161702226\n",
      "TRAIN: 0.9498591861747113\n",
      "TRAIN: 0.9502114890040665\n",
      "TRAIN: 0.9503280843419348\n",
      "TRAIN: 0.9504387802579466\n",
      "TRAIN: 0.9506920651849554\n",
      "TRAIN: 0.9511246515799432\n",
      "TRAIN: 0.9516653982583764\n",
      "TRAIN: 0.9523193943176258\n",
      "TRAIN: 0.9529038341733477\n",
      "TRAIN: 0.9527912847813753\n",
      "TRAIN: 0.9528920219441533\n",
      "TRAIN: 0.9535992228626815\n",
      "TRAIN: 0.9535925813320552\n",
      "TRAIN: 0.9537724700169471\n",
      "TRAIN: 0.9529471926928368\n",
      "TRAIN: 0.9534891073142335\n",
      "TRAIN: 0.9528114137036312\n",
      "TRAIN: 0.9523299020386068\n",
      "TRAIN: 0.9522531552217368\n",
      "TRAIN: 0.9523713021969236\n",
      "TRAIN: 0.9522765135323837\n",
      "TRAIN: 0.9521308281156179\n",
      "TRAIN: 0.9521474486396279\n",
      "TRAIN: 0.9519647439215647\n",
      "TRAIN: 0.9519438438359512\n",
      "TRAIN: 0.9518385467448677\n",
      "TRAIN: 0.9515069881671347\n",
      "TRAIN: 0.9515154145993082\n",
      "TRAIN: 0.9517675588707682\n",
      "TRAIN: 0.9514364294834126\n",
      "TRAIN: 0.9511227634390308\n",
      "TRAIN: 0.9515182505848696\n",
      "TRAIN: 0.9522489617345588\n",
      "TRAIN: 0.9533305616017802\n",
      "TRAIN: 0.9543557528739683\n",
      "TRAIN: 0.9552935012120536\n",
      "TRAIN: 0.9559919140743592\n",
      "TRAIN: 0.9569116495681212\n",
      "TRAIN: 0.9577797172626656\n",
      "TRAIN: 0.958570942630589\n",
      "TRAIN: 0.9588102571990608\n",
      "TRAIN: 0.9590351897560266\n",
      "TRAIN: 0.9591141478386125\n",
      "TRAIN: 0.9591321719098188\n",
      "TRAIN: 0.9591182106215689\n",
      "TRAIN: 0.9590044981925729\n",
      "TRAIN: 0.9590758127902995\n",
      "TRAIN: 0.9591067538583907\n",
      "TRAIN: 0.9592595760597123\n",
      "TRAIN: 0.9592938289919987\n",
      "TRAIN: 0.9593995088575883\n",
      "TRAIN: 0.9594897509190379\n",
      "TRAIN: 0.9595228028219154\n",
      "TRAIN: 0.9595655008626527\n",
      "TRAIN: 0.9595673198161353\n",
      "TRAIN: 0.9597461477659908\n",
      "TRAIN: 0.9597406416418957\n",
      "TRAIN: 0.9597851667080447\n",
      "TRAIN: 0.9598674458315609\n",
      "TRAIN: 0.959646850534183\n",
      "TRAIN: 0.9599649763350451\n",
      "TRAIN: 0.959938346936448\n",
      "TRAIN: 0.9599304091211105\n",
      "TRAIN: 0.9598788757759366\n",
      "TRAIN: 0.9599771825443641\n",
      "TRAIN: 0.9600319333106503\n",
      "TRAIN: 0.9600471211236081\n",
      "TRAIN: 0.9600926503181915\n",
      "TRAIN: 0.9601761527170762\n",
      "TRAIN: 0.9601928987603231\n",
      "TRAIN: 0.9601210767522739\n",
      "TRAIN: 0.9603657468904396\n",
      "TRAIN: 0.9609036104331854\n",
      "TRAIN: 0.9614276762512884\n",
      "TRAIN: 0.9619376425714018\n",
      "TRAIN: 0.9624344150680557\n",
      "TRAIN: 0.9629655305611526\n",
      "TRAIN: 0.9635036292151651\n",
      "TRAIN: 0.964026315020062\n",
      "TRAIN: 0.9639382475463496\n",
      "TRAIN: 0.9638229437286558\n",
      "TRAIN: 0.9638420769828916\n",
      "TRAIN: 0.9637698111482881\n",
      "TRAIN: 0.9637907153572226\n",
      "TRAIN: 0.9637775986061218\n",
      "TRAIN: 0.9634735813884249\n",
      "Epoch 1, step 21 ,acc: 0.948307\n",
      "0.9483072637668738\n",
      "['dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'dmul', 'dload_3', 'dload', 'dmul', 'dsub', 'dstore', 'dload_3', 'dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'iflt', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'ifge', 'aload_1', 'arraylength', 'iconst_1', 'isub', 'istore', 'aload_1', 'iload', 'aaload', 'invokevirtual', 'aload_1', 'iconst_0', 'aaload', 'invokevirtual', 'dsub', 'dstore', 'dload', 'dconst_0', 'dcmpl', 'ifne', 'new', 'dup', 'invokespecial', 'athrow', 'aload_2', 'iconst_1', 'ldc2_w', 'dload', 'ddiv', 'dastore', 'ldc2_w', 'dstore', 'ldc2_w', 'dstore']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "TRAIN: 0.9280742406845093\n",
      "TRAIN: 0.924389289025421\n",
      "TRAIN: 0.9150860811900842\n",
      "TRAIN: 0.9226031629389695\n",
      "TRAIN: 0.9220069576363864\n",
      "TRAIN: 0.9312154317879369\n",
      "TRAIN: 0.9367788515913372\n",
      "TRAIN: 0.9379098849831713\n",
      "TRAIN: 0.941429293695156\n",
      "TRAIN: 0.9436070898395892\n",
      "TRAIN: 0.9371537866094942\n",
      "TRAIN: 0.9360291495529387\n",
      "TRAIN: 0.9365699525298623\n",
      "TRAIN: 0.935119143745793\n",
      "TRAIN: 0.9372467658762192\n",
      "TRAIN: 0.9354185329671126\n",
      "TRAIN: 0.9372300220783492\n",
      "TRAIN: 0.9382178384779988\n",
      "TRAIN: 0.9407625348033457\n",
      "TRAIN: 0.9423524124075436\n",
      "TRAIN: 0.941992026962026\n",
      "TRAIN: 0.9410645069981434\n",
      "TRAIN: 0.9392657244363541\n",
      "TRAIN: 0.9392346847427363\n",
      "TRAIN: 0.939711494376476\n",
      "TRAIN: 0.9401564739181766\n",
      "TRAIN: 0.9406597513823971\n",
      "TRAIN: 0.9410348606347195\n",
      "TRAIN: 0.9413210254022327\n",
      "TRAIN: 0.9417866583592815\n",
      "TRAIN: 0.941801066738466\n",
      "TRAIN: 0.9402713415334353\n",
      "TRAIN: 0.9403053375244936\n",
      "TRAIN: 0.9411964988039041\n",
      "TRAIN: 0.9412065548116105\n",
      "TRAIN: 0.9413508125946588\n",
      "TRAIN: 0.9412682275928297\n",
      "TRAIN: 0.9416068978629317\n",
      "TRAIN: 0.942278490604194\n",
      "TRAIN: 0.9425109954838773\n",
      "TRAIN: 0.943191346972914\n",
      "TRAIN: 0.9437449567633532\n",
      "TRAIN: 0.9441123043887403\n",
      "TRAIN: 0.9443437609678481\n",
      "TRAIN: 0.9425311662037347\n",
      "TRAIN: 0.9417634827479782\n",
      "TRAIN: 0.9416705717218793\n",
      "TRAIN: 0.9422631371549366\n",
      "TRAIN: 0.9412169714771463\n",
      "TRAIN: 0.9418235925887624\n",
      "TRAIN: 0.9420070478409878\n",
      "TRAIN: 0.9421859327268953\n",
      "TRAIN: 0.9425937972403075\n",
      "TRAIN: 0.9425817091905847\n",
      "TRAIN: 0.942603688478561\n",
      "TRAIN: 0.9427239830321076\n",
      "TRAIN: 0.9420174024071624\n",
      "TRAIN: 0.9405783717601203\n",
      "TRAIN: 0.9409318785644084\n",
      "TRAIN: 0.9414944073510388\n",
      "TRAIN: 0.9411519114256857\n",
      "TRAIN: 0.941088993655438\n",
      "TRAIN: 0.9412293496366592\n",
      "TRAIN: 0.9414981277767589\n",
      "TRAIN: 0.9417257460574058\n",
      "TRAIN: 0.942012514045799\n",
      "TRAIN: 0.9421538350408987\n",
      "TRAIN: 0.9412800219145068\n",
      "TRAIN: 0.9411557048768265\n",
      "TRAIN: 0.9413154415170184\n",
      "TRAIN: 0.9415372130560046\n",
      "TRAIN: 0.9418075911595246\n",
      "TRAIN: 0.9420654530575239\n",
      "TRAIN: 0.9422067268241053\n",
      "TRAIN: 0.9423948919370021\n",
      "TRAIN: 0.9425497670948542\n",
      "TRAIN: 0.9427519684027431\n",
      "TRAIN: 0.9431937183890519\n",
      "TRAIN: 0.9434382397511741\n",
      "TRAIN: 0.9447640188208809\n",
      "TRAIN: 0.9474410942155249\n",
      "TRAIN: 0.9476846096157953\n",
      "TRAIN: 0.9498265602221543\n",
      "TRAIN: 0.9522258029254905\n",
      "TRAIN: 0.9542555110377804\n",
      "TRAIN: 0.9562583271562595\n",
      "TRAIN: 0.9563328836004766\n",
      "TRAIN: 0.9564541968861149\n",
      "TRAIN: 0.9566144592243772\n",
      "TRAIN: 0.956788274547033\n",
      "TRAIN: 0.9566846027080909\n",
      "TRAIN: 0.9567137466486115\n",
      "TRAIN: 0.9569456698554663\n",
      "TRAIN: 0.9570228123409991\n",
      "TRAIN: 0.9572311444598646\n",
      "TRAIN: 0.9560584063391937\n",
      "TRAIN: 0.9561562672000747\n",
      "TRAIN: 0.9564022313297439\n",
      "TRAIN: 0.9561145955462761\n",
      "TRAIN: 0.9559459586933394\n",
      "TRAIN: 0.9555205979576813\n",
      "TRAIN: 0.9555596103807934\n",
      "TRAIN: 0.9554713212499854\n",
      "TRAIN: 0.9554088029073147\n",
      "TRAIN: 0.9554051351837692\n",
      "TRAIN: 0.9550332558152076\n",
      "TRAIN: 0.9548943605305366\n",
      "TRAIN: 0.9550361729325935\n",
      "TRAIN: 0.9550796855713328\n",
      "TRAIN: 0.9551132526748384\n",
      "TRAIN: 0.9549942133869984\n",
      "TRAIN: 0.9550764215786987\n",
      "TRAIN: 0.9550241626855916\n",
      "TRAIN: 0.9551672264873882\n",
      "TRAIN: 0.9554385911208734\n",
      "TRAIN: 0.9555777247092192\n",
      "TRAIN: 0.9566610114177893\n",
      "TRAIN: 0.9566351671246758\n",
      "TRAIN: 0.9568456331975527\n",
      "TRAIN: 0.9569844812193014\n",
      "TRAIN: 0.9569887530862701\n",
      "TRAIN: 0.9567434810636531\n",
      "TRAIN: 0.9566234993038734\n",
      "TRAIN: 0.9568491045584745\n",
      "TRAIN: 0.9570228537598869\n",
      "TRAIN: 0.9572050919189509\n",
      "TRAIN: 0.9573659659507746\n",
      "TRAIN: 0.9576595648549503\n",
      "TRAIN: 0.9581028560461515\n",
      "TRAIN: 0.9586019299210914\n",
      "TRAIN: 0.959114188213084\n",
      "TRAIN: 0.9591504463021172\n",
      "TRAIN: 0.959507276497444\n",
      "TRAIN: 0.9600566934623717\n",
      "TRAIN: 0.9601540945755703\n",
      "TRAIN: 0.9602949081760808\n",
      "TRAIN: 0.9596568482601733\n",
      "TRAIN: 0.9600989902555226\n",
      "TRAIN: 0.9600887370636746\n",
      "TRAIN: 0.9599218227523266\n",
      "TRAIN: 0.9598401966278756\n",
      "TRAIN: 0.959906555843045\n",
      "TRAIN: 0.9597969716022727\n",
      "TRAIN: 0.9595995966730704\n",
      "TRAIN: 0.9596470976072564\n",
      "TRAIN: 0.9594650001820567\n",
      "TRAIN: 0.9594702940737379\n",
      "TRAIN: 0.9593522985968439\n",
      "TRAIN: 0.958980124082667\n",
      "TRAIN: 0.9589237760288135\n",
      "TRAIN: 0.9591608847211683\n",
      "TRAIN: 0.9588845668592824\n",
      "TRAIN: 0.9585395480461283\n",
      "TRAIN: 0.9588643854066717\n",
      "TRAIN: 0.9595137470584051\n",
      "TRAIN: 0.9604307936273718\n",
      "TRAIN: 0.9613000135368657\n",
      "TRAIN: 0.962095093974703\n",
      "TRAIN: 0.9626831748110689\n",
      "TRAIN: 0.9634630680492972\n",
      "TRAIN: 0.9641991493790828\n",
      "TRAIN: 0.9648700719633946\n",
      "TRAIN: 0.9650274274790096\n",
      "TRAIN: 0.9652666166960465\n",
      "TRAIN: 0.9653286410165397\n",
      "TRAIN: 0.9653609951608545\n",
      "TRAIN: 0.9654161723243437\n",
      "TRAIN: 0.9653013987098571\n",
      "TRAIN: 0.9653634949097316\n",
      "TRAIN: 0.9653911304090579\n",
      "TRAIN: 0.9655321700722614\n",
      "TRAIN: 0.9655713244889964\n",
      "TRAIN: 0.9657179804393722\n",
      "TRAIN: 0.9657949871176196\n",
      "TRAIN: 0.9658244006996091\n",
      "TRAIN: 0.9658452211571438\n",
      "TRAIN: 0.9659525519194783\n",
      "TRAIN: 0.9661279831229074\n",
      "TRAIN: 0.9661822774125951\n",
      "TRAIN: 0.9661951050057175\n",
      "TRAIN: 0.9662408425056958\n",
      "TRAIN: 0.9661055694012134\n",
      "TRAIN: 0.9663450295515305\n",
      "TRAIN: 0.966260286587771\n",
      "TRAIN: 0.9662194162079412\n",
      "TRAIN: 0.9661533588052805\n",
      "TRAIN: 0.9662256364697449\n",
      "TRAIN: 0.9663222798132601\n",
      "TRAIN: 0.9663297642030079\n",
      "TRAIN: 0.9663917357270515\n",
      "TRAIN: 0.9664808287817304\n",
      "TRAIN: 0.966535982288378\n",
      "TRAIN: 0.9664931273220891\n",
      "TRAIN: 0.9667043869634548\n",
      "TRAIN: 0.9671562308858375\n",
      "TRAIN: 0.9675964837306938\n",
      "TRAIN: 0.9680248919867855\n",
      "TRAIN: 0.9684422164856241\n",
      "TRAIN: 0.9688883915626966\n",
      "TRAIN: 0.9693404329953926\n",
      "TRAIN: 0.9697766002767783\n",
      "TRAIN: 0.9697116208628597\n",
      "TRAIN: 0.9696343188193224\n",
      "TRAIN: 0.9696495540346972\n",
      "TRAIN: 0.9695883428342131\n",
      "TRAIN: 0.9696003794596574\n",
      "TRAIN: 0.9695684951841396\n",
      "TRAIN: 0.9693142783372228\n",
      "Epoch 2, step 21 ,acc: 0.952861\n",
      "0.9528605099635741\n",
      "['dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'dmul', 'dload_3', 'dload', 'dmul', 'dsub', 'dstore', 'dload_3', 'dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'iflt', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'ifge', 'aload_1', 'arraylength', 'iconst_1', 'isub', 'istore', 'aload_1', 'iload', 'aaload', 'invokevirtual', 'aload_1', 'iconst_0', 'aaload', 'invokevirtual', 'dsub', 'dstore', 'dload', 'dconst_0', 'dcmpl', 'ifne', 'new', 'dup', 'invokespecial', 'athrow', 'aload_2', 'iconst_1', 'ldc2_w', 'dload', 'ddiv', 'dastore', 'ldc2_w', 'dstore', 'ldc2_w', 'dstore']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "TRAIN: 0.9292343258857727\n",
      "TRAIN: 0.9302054883528699\n",
      "TRAIN: 0.9241318838360499\n",
      "TRAIN: 0.932867991363848\n",
      "TRAIN: 0.9349229937457233\n",
      "TRAIN: 0.943554103298401\n",
      "TRAIN: 0.9490384614739854\n",
      "TRAIN: 0.9497567158068666\n",
      "TRAIN: 0.9512893973816593\n",
      "TRAIN: 0.9525770153353191\n",
      "TRAIN: 0.950606700786655\n",
      "TRAIN: 0.9477959609974415\n",
      "TRAIN: 0.9478885366840631\n",
      "TRAIN: 0.9451577105606302\n",
      "TRAIN: 0.9472773972678201\n",
      "TRAIN: 0.9446040790109888\n",
      "TRAIN: 0.9459593231951358\n",
      "TRAIN: 0.9465436992100243\n",
      "TRAIN: 0.9487138716941258\n",
      "TRAIN: 0.950078764957334\n",
      "TRAIN: 0.9500377161471752\n",
      "TRAIN: 0.9500969017784658\n",
      "TRAIN: 0.9487606636298173\n",
      "TRAIN: 0.9491542153746884\n",
      "TRAIN: 0.9497703631302542\n",
      "TRAIN: 0.9501197536361307\n",
      "TRAIN: 0.9510014662730584\n",
      "TRAIN: 0.9510299869713581\n",
      "TRAIN: 0.9515403996294736\n",
      "TRAIN: 0.9519835453715845\n",
      "TRAIN: 0.9520748454141116\n",
      "TRAIN: 0.9503639983060527\n",
      "TRAIN: 0.950294839539491\n",
      "TRAIN: 0.9510189128653338\n",
      "TRAIN: 0.9513848062927722\n",
      "TRAIN: 0.9514229041900732\n",
      "TRAIN: 0.951717138446844\n",
      "TRAIN: 0.9519286705792979\n",
      "TRAIN: 0.952673504377019\n",
      "TRAIN: 0.952875602936419\n",
      "TRAIN: 0.9535057704471886\n",
      "TRAIN: 0.953916122564514\n",
      "TRAIN: 0.9542314348746738\n",
      "TRAIN: 0.9544313209551766\n",
      "TRAIN: 0.9518658858114891\n",
      "TRAIN: 0.9510580926032977\n",
      "TRAIN: 0.9507431352222855\n",
      "TRAIN: 0.9511069929648512\n",
      "TRAIN: 0.94995180144972\n",
      "TRAIN: 0.9502726117807523\n",
      "TRAIN: 0.9505517667831612\n",
      "TRAIN: 0.9505962847230413\n",
      "TRAIN: 0.9510101483461078\n",
      "TRAIN: 0.950991553884633\n",
      "TRAIN: 0.9510737905839065\n",
      "TRAIN: 0.9513432788917091\n",
      "TRAIN: 0.9510033748857379\n",
      "TRAIN: 0.9504326488599909\n",
      "TRAIN: 0.9508906423477472\n",
      "TRAIN: 0.95133853003721\n",
      "TRAIN: 0.951121353685837\n",
      "TRAIN: 0.9513677815835874\n",
      "TRAIN: 0.95145521994691\n",
      "TRAIN: 0.9513884488631843\n",
      "TRAIN: 0.9515193438508605\n",
      "TRAIN: 0.9517419597483048\n",
      "TRAIN: 0.9517359989094091\n",
      "TRAIN: 0.9510219298528768\n",
      "TRAIN: 0.9508037586398214\n",
      "TRAIN: 0.950911476880864\n",
      "TRAIN: 0.951170122848627\n",
      "TRAIN: 0.9513604995927953\n",
      "TRAIN: 0.9515198034359417\n",
      "TRAIN: 0.9515496209689452\n",
      "TRAIN: 0.9516318639058292\n",
      "TRAIN: 0.951755114806602\n",
      "TRAIN: 0.9518171573027121\n",
      "TRAIN: 0.952090105950697\n",
      "TRAIN: 0.9522781253071783\n",
      "TRAIN: 0.9535625576388526\n",
      "TRAIN: 0.955813201724129\n",
      "TRAIN: 0.9559236426278798\n",
      "TRAIN: 0.9577282622571316\n",
      "TRAIN: 0.9597496536304555\n",
      "TRAIN: 0.9614597075832477\n",
      "TRAIN: 0.963147104701771\n",
      "TRAIN: 0.9631743683343456\n",
      "TRAIN: 0.9632319483555934\n",
      "TRAIN: 0.9634231054518031\n",
      "TRAIN: 0.9636330298459609\n",
      "TRAIN: 0.9634728386423845\n",
      "TRAIN: 0.9635122419413874\n",
      "TRAIN: 0.9636566444345692\n",
      "TRAIN: 0.9638015218561156\n",
      "TRAIN: 0.9637453747810523\n",
      "TRAIN: 0.9623687137029324\n",
      "TRAIN: 0.9623846944242731\n",
      "TRAIN: 0.9626032824496992\n",
      "TRAIN: 0.9623138392925961\n",
      "TRAIN: 0.9620369027992457\n",
      "TRAIN: 0.9615881610446922\n",
      "TRAIN: 0.9617801555035344\n",
      "TRAIN: 0.9616513215705634\n",
      "TRAIN: 0.9616731740845269\n",
      "TRAIN: 0.9616686334200907\n",
      "TRAIN: 0.9610778682978859\n",
      "TRAIN: 0.9608662224591099\n",
      "TRAIN: 0.9609862752765066\n",
      "TRAIN: 0.9611019386119826\n",
      "TRAIN: 0.9611257983224467\n",
      "TRAIN: 0.9609339537244455\n",
      "TRAIN: 0.9610478304988587\n",
      "TRAIN: 0.9608949354506998\n",
      "TRAIN: 0.9611772655593561\n",
      "TRAIN: 0.9611886764111615\n",
      "TRAIN: 0.9613210306770529\n",
      "TRAIN: 0.9622701452966904\n",
      "TRAIN: 0.9623558933466889\n",
      "TRAIN: 0.9625325929477572\n",
      "TRAIN: 0.9626532972515829\n",
      "TRAIN: 0.9627069748879699\n",
      "TRAIN: 0.9625517326170254\n",
      "TRAIN: 0.9624426137635901\n",
      "TRAIN: 0.9626140253083857\n",
      "TRAIN: 0.962760326857196\n",
      "TRAIN: 0.9628447324861694\n",
      "TRAIN: 0.9630406367926867\n",
      "TRAIN: 0.9632418540512475\n",
      "TRAIN: 0.9636462267862529\n",
      "TRAIN: 0.9640889153825013\n",
      "TRAIN: 0.964460396813912\n",
      "TRAIN: 0.9642389146145595\n",
      "TRAIN: 0.9644589844047984\n",
      "TRAIN: 0.9649271618614448\n",
      "TRAIN: 0.9650034403246259\n",
      "TRAIN: 0.965116779415911\n",
      "TRAIN: 0.964215156416113\n",
      "TRAIN: 0.964586146965661\n",
      "TRAIN: 0.9646448423048838\n",
      "TRAIN: 0.9644923678645773\n",
      "TRAIN: 0.9643914046324736\n",
      "TRAIN: 0.9644266981323907\n",
      "TRAIN: 0.9642941592818356\n",
      "TRAIN: 0.9641068749487692\n",
      "TRAIN: 0.9640882064553964\n",
      "TRAIN: 0.9638772101040826\n",
      "TRAIN: 0.9638828636992613\n",
      "TRAIN: 0.9637577594519292\n",
      "TRAIN: 0.9633806514046939\n",
      "TRAIN: 0.9632405384251194\n",
      "TRAIN: 0.9634182279974656\n",
      "TRAIN: 0.9630606127501313\n",
      "TRAIN: 0.962659988591062\n",
      "TRAIN: 0.9629240963335853\n",
      "TRAIN: 0.9634903616869059\n",
      "TRAIN: 0.9643173346991415\n",
      "TRAIN: 0.9651011786512094\n",
      "TRAIN: 0.9658181652109964\n",
      "TRAIN: 0.9663435701186017\n",
      "TRAIN: 0.9670469638814556\n",
      "TRAIN: 0.9677108432317891\n",
      "TRAIN: 0.9683159552369078\n",
      "TRAIN: 0.9684550457752915\n",
      "TRAIN: 0.9686424592361028\n",
      "TRAIN: 0.968700910641417\n",
      "TRAIN: 0.9687449829059839\n",
      "TRAIN: 0.9688097203361307\n",
      "TRAIN: 0.9688830231572638\n",
      "TRAIN: 0.9689393615495071\n",
      "TRAIN: 0.9689512864170843\n",
      "TRAIN: 0.9690921958921022\n",
      "TRAIN: 0.9691242812532241\n",
      "TRAIN: 0.9692298712161096\n",
      "TRAIN: 0.9693084298420604\n",
      "TRAIN: 0.9693932597285234\n",
      "TRAIN: 0.9694156232895417\n",
      "TRAIN: 0.9694630057732359\n",
      "TRAIN: 0.9696099566824851\n",
      "TRAIN: 0.9696766486103127\n",
      "TRAIN: 0.9696633563443167\n",
      "TRAIN: 0.9696836657538881\n",
      "TRAIN: 0.9695393665855186\n",
      "TRAIN: 0.9697548254557866\n",
      "TRAIN: 0.9696764520563814\n",
      "TRAIN: 0.969638094906001\n",
      "TRAIN: 0.9696279963659968\n",
      "TRAIN: 0.969678470173793\n",
      "TRAIN: 0.9697009852551208\n",
      "TRAIN: 0.9696360755485651\n",
      "TRAIN: 0.9696921980180065\n",
      "TRAIN: 0.9697689487075857\n",
      "TRAIN: 0.9698052141629661\n",
      "TRAIN: 0.9697895796189484\n",
      "TRAIN: 0.9699801073022577\n",
      "TRAIN: 0.9703874974906037\n",
      "TRAIN: 0.9707844369657218\n",
      "TRAIN: 0.971170697157555\n",
      "TRAIN: 0.9715469640447729\n",
      "TRAIN: 0.9719492431054835\n",
      "TRAIN: 0.9723568113724974\n",
      "TRAIN: 0.9727527055939493\n",
      "TRAIN: 0.9725895719754949\n",
      "TRAIN: 0.9723995751807882\n",
      "TRAIN: 0.9724173636736981\n",
      "TRAIN: 0.9723735062596587\n",
      "TRAIN: 0.972374676291759\n",
      "TRAIN: 0.972365383833576\n",
      "TRAIN: 0.9721080288377247\n",
      "Epoch 3, step 21 ,acc: 0.954414\n",
      "0.9544139704306835\n",
      "['dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'dmul', 'dload_3', 'dload', 'dmul', 'dsub', 'dstore', 'dload_3', 'dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'iflt', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'ifge', 'aload_1', 'arraylength', 'iconst_1', 'isub', 'istore', 'aload_1', 'iload', 'aaload', 'invokevirtual', 'aload_1', 'iconst_0', 'aaload', 'invokevirtual', 'dsub', 'dstore', 'dload', 'dconst_0', 'dcmpl', 'ifne', 'new', 'dup', 'invokespecial', 'athrow', 'aload_2', 'iconst_1', 'ldc2_w', 'dload', 'ddiv', 'dastore', 'ldc2_w', 'dstore', 'ldc2_w', 'dstore']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "TRAIN: 0.9396751523017883\n",
      "TRAIN: 0.9406746708870674\n",
      "TRAIN: 0.9314268990059723\n",
      "TRAIN: 0.9404639737289364\n",
      "TRAIN: 0.9432025185225914\n",
      "TRAIN: 0.9496525356218741\n",
      "TRAIN: 0.9526442327536643\n",
      "TRAIN: 0.9541992839116084\n",
      "TRAIN: 0.9563458632372009\n",
      "TRAIN: 0.9575766500686251\n",
      "TRAIN: 0.9532445264669183\n",
      "TRAIN: 0.9510811157066211\n",
      "TRAIN: 0.9512783706342307\n",
      "TRAIN: 0.9496486545736479\n",
      "TRAIN: 0.9512303631341508\n",
      "TRAIN: 0.9485138288388357\n",
      "TRAIN: 0.9497390229810134\n",
      "TRAIN: 0.9503437120324749\n",
      "TRAIN: 0.952451004160378\n",
      "TRAIN: 0.9539044385539932\n",
      "TRAIN: 0.9534140338475007\n",
      "TRAIN: 0.9524155628464184\n",
      "TRAIN: 0.9502931801832593\n",
      "TRAIN: 0.9507636754515528\n",
      "TRAIN: 0.9513228570314542\n",
      "TRAIN: 0.9515886984217116\n",
      "TRAIN: 0.9522474563682881\n",
      "TRAIN: 0.9526145811279421\n",
      "TRAIN: 0.9531587178195245\n",
      "TRAIN: 0.9536291540387165\n",
      "TRAIN: 0.9540836855641724\n",
      "TRAIN: 0.9523218649850627\n",
      "TRAIN: 0.9522873563904373\n",
      "TRAIN: 0.9529310101513915\n",
      "TRAIN: 0.9531238040963582\n",
      "TRAIN: 0.9533820642348383\n",
      "TRAIN: 0.9535246517244585\n",
      "TRAIN: 0.9535762770439008\n",
      "TRAIN: 0.9543553956514172\n",
      "TRAIN: 0.9547412335758705\n",
      "TRAIN: 0.9552512893738804\n",
      "TRAIN: 0.955641189246411\n",
      "TRAIN: 0.9559806943095495\n",
      "TRAIN: 0.9563745955311655\n",
      "TRAIN: 0.9544271138444989\n",
      "TRAIN: 0.9539626589417458\n",
      "TRAIN: 0.9537809164327847\n",
      "TRAIN: 0.9542698314998312\n",
      "TRAIN: 0.9533158933125983\n",
      "TRAIN: 0.953907431644739\n",
      "TRAIN: 0.954057785421233\n",
      "TRAIN: 0.9540927247277791\n",
      "TRAIN: 0.954399034602051\n",
      "TRAIN: 0.9543518204079189\n",
      "TRAIN: 0.9542614646510705\n",
      "TRAIN: 0.954420314871782\n",
      "TRAIN: 0.9540756551712577\n",
      "TRAIN: 0.953758033145646\n",
      "TRAIN: 0.9541075637660349\n",
      "TRAIN: 0.9545408360001113\n",
      "TRAIN: 0.9542942094179369\n",
      "TRAIN: 0.9540641258798261\n",
      "TRAIN: 0.9541842605779725\n",
      "TRAIN: 0.9541938776083152\n",
      "TRAIN: 0.9542703555511161\n",
      "TRAIN: 0.954403937301829\n",
      "TRAIN: 0.9543772383535145\n",
      "TRAIN: 0.9543935565606669\n",
      "TRAIN: 0.9541276322477394\n",
      "TRAIN: 0.9542073808946188\n",
      "TRAIN: 0.9544678773141417\n",
      "TRAIN: 0.9547169283360505\n",
      "TRAIN: 0.9549461188547246\n",
      "TRAIN: 0.9551343587386469\n",
      "TRAIN: 0.9554945993368739\n",
      "TRAIN: 0.9557616608099091\n",
      "TRAIN: 0.955992639668973\n",
      "TRAIN: 0.9562526371563586\n",
      "TRAIN: 0.9564483929469721\n",
      "TRAIN: 0.9575892298162989\n",
      "TRAIN: 0.9596447166005113\n",
      "TRAIN: 0.9597489076787101\n",
      "TRAIN: 0.961396909365614\n",
      "TRAIN: 0.9632428697769564\n",
      "TRAIN: 0.964804512880698\n",
      "TRAIN: 0.9663454654741018\n",
      "TRAIN: 0.9664145427526241\n",
      "TRAIN: 0.9664278799949233\n",
      "TRAIN: 0.9665632993753993\n",
      "TRAIN: 0.9666161446753277\n",
      "TRAIN: 0.9665056766422533\n",
      "TRAIN: 0.9665913379416852\n",
      "TRAIN: 0.9666310364192717\n",
      "TRAIN: 0.9665057445109441\n",
      "TRAIN: 0.9664354605997145\n",
      "TRAIN: 0.9649218683416456\n",
      "TRAIN: 0.9649165743602933\n",
      "TRAIN: 0.9650787342193378\n",
      "TRAIN: 0.964865505998063\n",
      "TRAIN: 0.9647583871477826\n",
      "TRAIN: 0.9642768283443786\n",
      "TRAIN: 0.9641842672932005\n",
      "TRAIN: 0.9637995310549501\n",
      "TRAIN: 0.9638024407703094\n",
      "TRAIN: 0.9638585579620937\n",
      "TRAIN: 0.9634972275373448\n",
      "TRAIN: 0.9633394154766903\n",
      "TRAIN: 0.9634053802807632\n",
      "TRAIN: 0.963463018183274\n",
      "TRAIN: 0.9632509198277299\n",
      "TRAIN: 0.9630432553142652\n",
      "TRAIN: 0.9630165050844239\n",
      "TRAIN: 0.9627993557733523\n",
      "TRAIN: 0.9627658831603286\n",
      "TRAIN: 0.9626766050503608\n",
      "TRAIN: 0.9628036195881404\n",
      "TRAIN: 0.9637095954068498\n",
      "TRAIN: 0.9637843960403962\n",
      "TRAIN: 0.9638894797093008\n",
      "TRAIN: 0.9640225424366469\n",
      "TRAIN: 0.964025049646345\n",
      "TRAIN: 0.9637731529235413\n",
      "TRAIN: 0.963625724504113\n",
      "TRAIN: 0.9637415275172843\n",
      "TRAIN: 0.9638761210650179\n",
      "TRAIN: 0.9639588887356081\n",
      "TRAIN: 0.964101551929797\n",
      "TRAIN: 0.9642490915769619\n",
      "TRAIN: 0.9646356872012332\n",
      "TRAIN: 0.9650599583476982\n",
      "TRAIN: 0.9654340042828443\n",
      "TRAIN: 0.9651635730300586\n",
      "TRAIN: 0.9652100853937333\n",
      "TRAIN: 0.9657480265059896\n",
      "TRAIN: 0.9658432706095136\n",
      "TRAIN: 0.96596168278044\n",
      "TRAIN: 0.9650831515321854\n",
      "TRAIN: 0.9654373393979224\n",
      "TRAIN: 0.9652616511060268\n",
      "TRAIN: 0.9647283310456494\n",
      "TRAIN: 0.9644832431803534\n",
      "TRAIN: 0.964446965670658\n",
      "TRAIN: 0.9642488831347956\n",
      "TRAIN: 0.9640268321542972\n",
      "TRAIN: 0.9639539266521203\n",
      "TRAIN: 0.9636894546083895\n",
      "TRAIN: 0.9636418184272669\n",
      "TRAIN: 0.9634885348045107\n",
      "TRAIN: 0.9631771007198022\n",
      "TRAIN: 0.9630251765986544\n",
      "TRAIN: 0.9632139109589806\n",
      "TRAIN: 0.9627310462698911\n",
      "TRAIN: 0.9622037108351775\n",
      "TRAIN: 0.9624453969953263\n",
      "TRAIN: 0.9630012072866718\n",
      "TRAIN: 0.9638392600440514\n",
      "TRAIN: 0.9646336059001634\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-5a21c9bc39f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0monehot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                     \u001b[0monehot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                     \u001b[0mfilled_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0monehot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0mmask_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "N = int(0.9*X_train.shape[0])\n",
    "print(N)\n",
    "testN = X_train.shape[0]-N\n",
    "\n",
    "assert N+testN == X_train.shape[0]\n",
    "n_steps = N//batch_size\n",
    "\n",
    "new_Y = []\n",
    "ln, acsum = 0,0\n",
    "for y_seq in Y_train:\n",
    "    y_seq = list(map(lambda x: ((x[0],1) if x[1]==1 else (x[0],0)), y_seq))\n",
    "    new_Y.append(y_seq)\n",
    "    acsum += sum(list(map((lambda x:x[1]), y_seq)))\n",
    "    ln += len(y_seq)\n",
    "Y_train = new_Y\n",
    "print('BASELINE:', 1-acsum/ln)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "from random import shuffle\n",
    "indices = np.arange(X_train.shape[0])\n",
    "shuffle(indices)\n",
    "\n",
    "X_train, X_test = X_train[indices[:N]], X_train[indices[N:N+testN]]\n",
    "Y_train, Y_test = Y_train[indices[:N]], Y_train[indices[N:N+testN]]\n",
    "\n",
    "n_steps_test = int(testN/batch_size)\n",
    "\"\"\"\n",
    "X_train, Y_train = data.Data.flattenData(part_1.X, part_1.Y, sequence_length)\n",
    "X_test, Y_test = data.Data.flattenData(part_2.X, part_2.Y, sequence_length)\n",
    "\n",
    "N=len(X_train)\n",
    "print(N)\n",
    "print(part_1.baseline)\n",
    "print(part_2.baseline)\n",
    "n_steps = int(len(X_train)/batch_size)\n",
    "n_steps_test = int(len(X_test)/batch_size)\n",
    "with tf.Session(graph=rnn2_graph) as session:\n",
    "    iop = tf.global_variables_initializer()\n",
    "    loc = tf.local_variables_initializer()\n",
    "    session.run(iop)\n",
    "    #session.run(loc)\n",
    "    for epoch in range(num_epochs):\n",
    "        acc_sum=0\n",
    "        batch_index = 0\n",
    "        train_acc = 0\n",
    "        div_sum = 0\n",
    "        for step in range(n_steps):\n",
    "            cur_batch_size = 0\n",
    "            batch = []\n",
    "            batch_Y = []\n",
    "            sequence_len = []\n",
    "            \n",
    "            while(cur_batch_size < batch_size):\n",
    "                cur_seq = []\n",
    "                y_cur_seq = []\n",
    "                for x,y in zip(X_train[batch_index],Y_train[batch_index]):\n",
    "                    if x in ByteCode.mnemonicMap:\n",
    "                        cur_seq.append(x)\n",
    "                        y_cur_seq.append(y)\n",
    "                    if len(cur_seq) == sequence_length:\n",
    "                        batch.append(cur_seq)\n",
    "                        batch_Y.append(y_cur_seq)\n",
    "                        sequence_len.append(len(y_cur_seq))\n",
    "                        cur_batch_size += 1\n",
    "                        cur_seq = []\n",
    "                        y_cur_seq = []\n",
    "                if cur_seq:\n",
    "                    batch.append(cur_seq)\n",
    "                    batch_Y.append(y_cur_seq)\n",
    "                    sequence_len.append(len(y_cur_seq))\n",
    "                    cur_batch_size += 1\n",
    "                batch_index += 1\n",
    "            \n",
    "            data = np.zeros([batch_size, sequence_length])\n",
    "            filled_labels = np.zeros([batch_size, sequence_length, output_size])\n",
    "            filled_labels[:,:,0] = 0\n",
    "            mask_val = np.zeros([batch_size, sequence_length, output_size])\n",
    "            for i, seq in enumerate(batch_Y):\n",
    "                for j, label in enumerate(seq):\n",
    "                    onehot=np.zeros(output_size)\n",
    "                    onehot[label[1]] = 1\n",
    "                    filled_labels[i,j,:] = onehot\n",
    "                    mask_val[i,j,:] = 1\n",
    "            for i in range(batch_size):\n",
    "                for j in range(sequence_len[i]):\n",
    "                    data[i,j] = dictionary.get(batch[i][j],0)+1\n",
    "            \n",
    "            feed = {sequence:data, labels:filled_labels, seq_len:sequence_len, mask:mask_val}\n",
    "            ops = [logits, diff, accuracy, train_step]\n",
    "            logits_val, diff_val, acc_fixed, _= session.run(ops, feed_dict=feed)\n",
    "            #acc_fixed = session.run([accuracy], feed_dict={seq_len:sequence_len})\n",
    "            #session.run(loc)\n",
    "            train_acc += acc_fixed*np.sum(sequence_len)\n",
    "            div_sum += np.sum(sequence_len)\n",
    "            \n",
    "            #print(acc_fixed)\n",
    "            #print(diff_val)\n",
    "            #print(np.sum(filled_labels))\n",
    "            #print(np.sum(sequence_len))\n",
    "            #print(np.argmax(filled_labels,axis=2)[0][:sequence_len[0]])\n",
    "            #print(np.argmax(logits_val,axis=2)[0][:sequence_len[0]])\n",
    "            '''\n",
    "            zum = 0\n",
    "            for i,sl in enumerate(sequence_len):\n",
    "                zum += np.sum(np.argmax(logits_val[i,sl:,:],axis=1))\n",
    "            print(zum)\n",
    "            '''\n",
    "            print('TRAIN:',train_acc/div_sum)\n",
    "        \n",
    "\n",
    "        acc_sum=0\n",
    "        num=0\n",
    "        acc_pom=0\n",
    "        acc_num = 0\n",
    "            \n",
    "        batch_index = 0\n",
    "        for step in range(n_steps_test):\n",
    "            cur_batch_size = 0\n",
    "            batch = []\n",
    "            batch_Y = []\n",
    "            sequence_len = []\n",
    "            while(cur_batch_size < batch_size):\n",
    "                cur_seq = []\n",
    "                y_cur_seq = []\n",
    "                for x,y in zip(X_test[batch_index],Y_test[batch_index]):\n",
    "                    if x in ByteCode.mnemonicMap:\n",
    "                        cur_seq.append(x)\n",
    "                        y_cur_seq.append(y)\n",
    "                    if len(cur_seq) == sequence_length:\n",
    "                        batch.append(cur_seq)\n",
    "                        batch_Y.append(y_cur_seq)\n",
    "                        sequence_len.append(len(y_cur_seq))\n",
    "                        cur_batch_size += 1\n",
    "                        cur_seq = []\n",
    "                        y_cur_seq = []\n",
    "                if cur_seq:\n",
    "                    batch.append(cur_seq)\n",
    "                    batch_Y.append(y_cur_seq)\n",
    "                    sequence_len.append(len(y_cur_seq))\n",
    "                    cur_batch_size += 1\n",
    "                batch_index += 1\n",
    "                \n",
    "            data = np.zeros([batch_size, sequence_length])\n",
    "            filled_labels = np.zeros([batch_size, sequence_length, output_size])\n",
    "            filled_labels[:,:,0] = 0\n",
    "            mask_val = np.zeros([batch_size, sequence_length, output_size])\n",
    "            for i, seq in enumerate(batch_Y):\n",
    "                for j, label in enumerate(seq):\n",
    "                    onehot=np.zeros(output_size)\n",
    "                    onehot[label[1]] = 1\n",
    "                    filled_labels[i,j,:] = onehot\n",
    "                    mask_val[i,j,:] = 1\n",
    "            for i in range(batch_size):\n",
    "                for j in range(sequence_len[i]):\n",
    "                    data[i,j] = dictionary.get(batch[i][j],0)+1\n",
    "            \n",
    "            feed = {sequence:data, labels:filled_labels, seq_len:sequence_len, mask:mask_val}\n",
    "            ops = [logits, diff, accuracy]\n",
    "            logits_val, diff_val, acc_fixed = session.run(ops, feed_dict=feed)\n",
    "            acc_sum += acc_fixed*np.sum(sequence_len)\n",
    "            acc_num += np.sum(sequence_len)\n",
    "            for logit_gold, logit_pred, sl in zip(filled_labels, logits_val, sequence_len):\n",
    "                acc_pom += np.sum((logit_gold[:sl,1].astype(np.int32)==np.argmax(logit_pred,axis=1)[:sl]))\n",
    "                num += sl\n",
    "                #print(logit_gold[:sl,1].astype(np.int32))\n",
    "                #print(np.argmax(logit_pred,axis=1)[:sl])\n",
    "                #print(acc_pom/num)\n",
    "                \n",
    "            \n",
    "        print(\"Epoch %d, step %d ,acc: %f\" % (epoch, step, acc_sum/acc_num))\n",
    "        print(acc_pom/num)\n",
    "        index = np.argmax(np.sum(np.argmax(filled_labels, axis=2),axis=1))\n",
    "        print(batch[index])\n",
    "        print(np.argmax(filled_labels,axis=2)[index][:sequence_len[index]])\n",
    "        print(np.argmax(logits_val,axis=2)[index][:sequence_len[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
