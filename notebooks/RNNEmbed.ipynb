{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yolkin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.loadtxt(os.path.join(dir_path, 'cache', 'embeddings.vec'))\n",
    "\n",
    "pickle_in = open(os.path.join(dir_path,'cache', 'database.dict'), 'rb')\n",
    "db = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open(os.path.join(dir_path,'cache','data2onehot.dict'), 'rb')\n",
    "dictionary = pickle.load(pickle_in)\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate data by length and by each label in one class 'Y_labels'\n",
    "# and by jump instruction 'Y_if'\n",
    "#\n",
    "from JavaClassParser import ByteCode\n",
    "reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
    "X_train=[]\n",
    "Y_train=[]\n",
    "X_train_long=[]\n",
    "Y_train_long=[]\n",
    "X_labels = []\n",
    "Y_labels = []\n",
    "\n",
    "X_if = []\n",
    "Y_if = []\n",
    "\n",
    "for dclass in db.values():\n",
    "    for method in dclass.values():\n",
    "        instructions = method['x']\n",
    "        labels = method['y']\n",
    "        byteIndex = method['index']\n",
    "        \n",
    "        \n",
    "        #\n",
    "        #seperating by labels\n",
    "\n",
    "        cur_section = []\n",
    "        cur_label = labels[0]\n",
    "        if len(instructions)<100:\n",
    "            X_train.append(instructions)\n",
    "            Y_train.append(labels)\n",
    "        else:\n",
    "            X_train_long.append(instructions)\n",
    "            Y_train_long.append(labels)\n",
    "        for instruction, label in zip(instructions,labels):\n",
    "            if label != cur_label:\n",
    "                X_labels.append(cur_section)\n",
    "                Y_labels.append(cur_label)\n",
    "                cur_section = []\n",
    "                cur_label = label\n",
    "            cur_section.append(dictionary.get(instruction,0))\n",
    "        \n",
    "        #print(instructions)\n",
    "        \n",
    "        \n",
    "        #\n",
    "        #seperating by if jumps\n",
    "        \n",
    "        for i,label in zip(range(len(instructions)),labels):\n",
    "        #index = method[i]\n",
    "        #instruction = reverse_dictionary[index]\n",
    "            instruction = instructions[i]\n",
    "            if 'if' in instruction[:2]:\n",
    "                #offset = int(reverse_dictionary[method[i+1]])\n",
    "                offset = int(instructions[i+1])\n",
    "                j = i\n",
    "                step = np.sign(offset)\n",
    "                curr = byteIndex[i]\n",
    "                end = curr+offset\n",
    "                section = []\n",
    "                while(curr!=end):\n",
    "                    section.append(dictionary.get(instructions[j],0))\n",
    "                    if instructions[j] in ByteCode.mnemonicMap:\n",
    "                        #curr += ByteCode.mnemonicMap[instructions[j]].getOpCodeCount() + 1\n",
    "                        curr = byteIndex[j]\n",
    "                    j += step\n",
    "                    if curr>=abs(offset):\n",
    "                        argCounts = ByteCode.mnemonicMap[instructions[j-step]].argsCount\n",
    "                        for arg in argCounts:\n",
    "                            if arg > 0:\n",
    "                                section.append(dictionary.get(instructions[j],0))\n",
    "                                j += step\n",
    "                        break\n",
    "\n",
    "                if offset<0:\n",
    "                    section = list(reversed(section))\n",
    "                    section.append(dictionary.get(instructions[i+1],0))\n",
    "                \n",
    "                X_if.append(section)\n",
    "                Y_if.append(label)\n",
    "                \n",
    "        #'''\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_train_long = np.array(X_train_long)\n",
    "Y_train_long = np.array(Y_train_long)\n",
    "X_labels = np.array(X_labels)\n",
    "Y_labels = np.array(Y_labels)\n",
    "X_if = np.array(X_if)\n",
    "Y_if = np.array(Y_if)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_size = np.max(Y_labels[:,1])+1\n",
    "output_size = 2\n",
    "learning_rate = 1e-2\n",
    "batch_size = 64\n",
    "hidden_size = 8\n",
    "'''\n",
    "num_features depends how do u we want to represent data\n",
    "we can use w2v embbeding to send dense represetation\n",
    "#num_features =embeddings.shape[1]\n",
    "we can use sparse representation which for this example requires over 5000 long one hot vector\n",
    "we can use sparse representation taking only instruction without arguments lowering one hot to 203 dim\n",
    "\n",
    "in this simple model dense representation didnt show better result then filtered representation \n",
    "with only instructions\n",
    "'''\n",
    "\n",
    "#num_features = len(ByteCode.mnemonicMap)\n",
    "num_features =embeddings.shape[0] + 1\n",
    "num_epochs = 15\n",
    "\n",
    "sequence_length = 100\n",
    "\n",
    "rnn2_graph = tf.Graph()\n",
    "\n",
    "with rnn2_graph.as_default():\n",
    "    \n",
    "    sequence = tf.placeholder(tf.int32,[batch_size, sequence_length])\n",
    "    labels= tf.placeholder(tf.float64,[batch_size, sequence_length, output_size])\n",
    "    seq_len = tf.placeholder(tf.int64, [batch_size])\n",
    "    mask = tf.placeholder(tf.float64, [batch_size, sequence_length, output_size])\n",
    "\n",
    "    cells_fw = [\n",
    "        tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.contrib.rnn.BasicLSTMCell(hidden_size,activation=tf.nn.tanh),\n",
    "        output_keep_prob = 0.8)\n",
    "        for _ in range(1)]\n",
    "    \n",
    "    cells_bw = [\n",
    "        tf.nn.rnn_cell.DropoutWrapper(\n",
    "        tf.contrib.rnn.BasicLSTMCell(hidden_size,activation=tf.nn.tanh),\n",
    "        output_keep_prob = 0.8)\n",
    "        for _ in range(1)]\n",
    "    \n",
    "    W_embed = tf.get_variable(\n",
    "         'W', shape=(num_features, hidden_size), initializer=tf.contrib.layers.xavier_initializer(), dtype=tf.float64)\n",
    "    \n",
    "    \n",
    "    W_1 = tf.Variable(tf.random_normal([1,hidden_size*2, hidden_size*2], dtype=tf.float64))\n",
    "    b_1 = tf.Variable(tf.random_normal([hidden_size*2],dtype=tf.float64))\n",
    "    W_2 = tf.Variable(tf.random_normal([1,hidden_size*2, output_size], dtype=tf.float64))\n",
    "    b_2 = tf.Variable(tf.random_normal([output_size], dtype=tf.float64))\n",
    "\n",
    "    #initial_state = cell_fw.zero_state(batch_size, dtype=tf.float64)\n",
    "\n",
    "\n",
    "    #outputs, state = tf.nn.dynamic_rnn(cell, sequence, \n",
    "    #        initial_state=initial_state, sequence_length=seq_len)\n",
    "\n",
    "    embed_input = tf.nn.embedding_lookup(W_embed, sequence, name = \"embedded_input\")\n",
    "    \n",
    "    outputs, states_fw, states_bw = tf.contrib.rnn.stack_bidirectional_dynamic_rnn(\n",
    "        cells_fw = cells_fw,\n",
    "        cells_bw=cells_bw,\n",
    "        dtype = tf.float64,\n",
    "        sequence_length = seq_len,\n",
    "        inputs = embed_input)\n",
    "    \n",
    "    #outputs = tf.concat(outputs, 2)\n",
    "    \n",
    "    outputs_2 = tf.nn.relu(tf.matmul(outputs, tf.tile(W_1, [tf.shape(outputs)[0],1,1])) + b_1)\n",
    "    logits  = tf.matmul(outputs, tf.tile(W_2, [tf.shape(outputs)[0],1,1])) + b_2\n",
    "    logits = logits*mask\n",
    "    '''\n",
    "    for output_batch, label_batch in zip(tf.unstack(outputs, axis=1), tf.unstack(labels, axis=1)):\n",
    "        for output, label in zip(tf.unstack(output_batch, axis=0), tf.unstack(label_batch, axis=0)):\n",
    "            output = tf.expand_dims(output, 0)\n",
    "            label = tf.expand_dims(label, 0)\n",
    "            logits = tf.matmul(output, W)+ b\n",
    "            loss += tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits)\n",
    "            correct_predictions += tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1)), tf.float64))\n",
    "        #incorrect_prediciton += batch_size - tf.reduce_sum(tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(label, 1)),tf.float64))\n",
    "    '''\n",
    "    loss = tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits = logits)\n",
    "\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    #accuracy = correct_predictions/tf.reduce_sum(tf.cast(seq_len,tf.float64))\n",
    "    #acc_val, acc_op = tf.metrics.accuracy(tf.argmax(labels,axis=2), tf.argmax(logits, axis=2))\n",
    "    diff = batch_size*sequence_length - tf.reduce_sum(tf.cast(tf.equal(tf.argmax(labels, axis =2), tf.argmax(logits, axis=2)), tf.float32))\n",
    "    nonpadsum = tf.cast(tf.reduce_sum(seq_len),tf.float32)\n",
    "    accuracy = (nonpadsum- diff)/nonpadsum\n",
    "    #correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "    #accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11269\n",
      "10142\n",
      "1126\n"
     ]
    }
   ],
   "source": [
    "from Model import data\n",
    "db = data.Data()\n",
    "db.loadDataFromJSON()\n",
    "db.relabelData(overwrite=True)\n",
    "part_1 = db.getPartition(0.9)\n",
    "part_2 = db.getPartition(0.1)\n",
    "dictionary = part_1.getVocabulary()\n",
    "#X_train, Y_train = data.Data.flattenData(part.X, part.Y, sequence_length)\n",
    "#X_train, Y_train = part.X, part.Y\n",
    "#X_train = np.array(X_train)\n",
    "#Y_train = np.array(Y_train)\n",
    "print(len(db.X))\n",
    "print(len(part_1.X))\n",
    "print(len(part_2.X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1470\n",
      "13337\n",
      "0.9253762289643525\n",
      "0.8913384234211458\n",
      "TRAIN: 0.946153475502579\n",
      "Epoch 0, step 21 ,acc: 0.940192\n",
      "0.9401917720162846\n",
      "['dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'dmul', 'dload_3', 'dload', 'dmul', 'dsub', 'dstore', 'dload_3', 'dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'iflt', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'ifge', 'aload_1', 'arraylength', 'iconst_1', 'isub', 'istore', 'aload_1', 'iload', 'aaload', 'invokevirtual', 'aload_1', 'iconst_0', 'aaload', 'invokevirtual', 'dsub', 'dstore', 'dload', 'dconst_0', 'dcmpl', 'ifne', 'new', 'dup', 'invokespecial', 'athrow', 'aload_2', 'iconst_1', 'ldc2_w', 'dload', 'ddiv', 'dastore', 'ldc2_w', 'dstore', 'ldc2_w', 'dstore']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "TRAIN: 0.9689984885150551\n",
      "Epoch 1, step 21 ,acc: 0.955834\n",
      "0.9558335118920077\n",
      "['dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'dmul', 'dload_3', 'dload', 'dmul', 'dsub', 'dstore', 'dload_3', 'dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'iflt', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'ifge', 'aload_1', 'arraylength', 'iconst_1', 'isub', 'istore', 'aload_1', 'iload', 'aaload', 'invokevirtual', 'aload_1', 'iconst_0', 'aaload', 'invokevirtual', 'dsub', 'dstore', 'dload', 'dconst_0', 'dcmpl', 'ifne', 'new', 'dup', 'invokespecial', 'athrow', 'aload_2', 'iconst_1', 'ldc2_w', 'dload', 'ddiv', 'dastore', 'ldc2_w', 'dstore', 'ldc2_w', 'dstore']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "TRAIN: 0.9741450189244184\n",
      "Epoch 2, step 21 ,acc: 0.957896\n",
      "0.9578958645811013\n",
      "['dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'dmul', 'dload_3', 'dload', 'dmul', 'dsub', 'dstore', 'dload_3', 'dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'iflt', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'ifge', 'aload_1', 'arraylength', 'iconst_1', 'isub', 'istore', 'aload_1', 'iload', 'aaload', 'invokevirtual', 'aload_1', 'iconst_0', 'aaload', 'invokevirtual', 'dsub', 'dstore', 'dload', 'dconst_0', 'dcmpl', 'ifne', 'new', 'dup', 'invokespecial', 'athrow', 'aload_2', 'iconst_1', 'ldc2_w', 'dload', 'ddiv', 'dastore', 'ldc2_w', 'dstore', 'ldc2_w', 'dstore']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "TRAIN: 0.976876180068812\n",
      "Epoch 3, step 21 ,acc: 0.960065\n",
      "0.9600653524748233\n",
      "['dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'dmul', 'dload_3', 'dload', 'dmul', 'dsub', 'dstore', 'dload_3', 'dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'iflt', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'ifge', 'aload_1', 'arraylength', 'iconst_1', 'isub', 'istore', 'aload_1', 'iload', 'aaload', 'invokevirtual', 'aload_1', 'iconst_0', 'aaload', 'invokevirtual', 'dsub', 'dstore', 'dload', 'dconst_0', 'dcmpl', 'ifne', 'new', 'dup', 'invokespecial', 'athrow', 'aload_2', 'iconst_1', 'ldc2_w', 'dload', 'ddiv', 'dastore', 'ldc2_w', 'dstore', 'ldc2_w', 'dstore']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "TRAIN: 0.9788050607449513\n",
      "Epoch 4, step 21 ,acc: 0.962690\n",
      "0.9626901649882151\n",
      "['dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'dmul', 'dload_3', 'dload', 'dmul', 'dsub', 'dstore', 'dload_3', 'dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'iflt', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'ifge', 'aload_1', 'arraylength', 'iconst_1', 'isub', 'istore', 'aload_1', 'iload', 'aaload', 'invokevirtual', 'aload_1', 'iconst_0', 'aaload', 'invokevirtual', 'dsub', 'dstore', 'dload', 'dconst_0', 'dcmpl', 'ifne', 'new', 'dup', 'invokespecial', 'athrow', 'aload_2', 'iconst_1', 'ldc2_w', 'dload', 'ddiv', 'dastore', 'ldc2_w', 'dstore', 'ldc2_w', 'dstore']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "TRAIN: 0.9800767567631606\n",
      "Epoch 5, step 21 ,acc: 0.963547\n",
      "0.963547246625241\n",
      "['dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'dmul', 'dload_3', 'dload', 'dmul', 'dsub', 'dstore', 'dload_3', 'dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'iflt', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'ifge', 'aload_1', 'arraylength', 'iconst_1', 'isub', 'istore', 'aload_1', 'iload', 'aaload', 'invokevirtual', 'aload_1', 'iconst_0', 'aaload', 'invokevirtual', 'dsub', 'dstore', 'dload', 'dconst_0', 'dcmpl', 'ifne', 'new', 'dup', 'invokespecial', 'athrow', 'aload_2', 'iconst_1', 'ldc2_w', 'dload', 'ddiv', 'dastore', 'ldc2_w', 'dstore', 'ldc2_w', 'dstore']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "TRAIN: 0.9807083360476627\n",
      "Epoch 6, step 21 ,acc: 0.963145\n",
      "0.9631454896078852\n",
      "['dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'dmul', 'dload_3', 'dload', 'dmul', 'dsub', 'dstore', 'dload_3', 'dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'iflt', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'ifge', 'aload_1', 'arraylength', 'iconst_1', 'isub', 'istore', 'aload_1', 'iload', 'aaload', 'invokevirtual', 'aload_1', 'iconst_0', 'aaload', 'invokevirtual', 'dsub', 'dstore', 'dload', 'dconst_0', 'dcmpl', 'ifne', 'new', 'dup', 'invokespecial', 'athrow', 'aload_2', 'iconst_1', 'ldc2_w', 'dload', 'ddiv', 'dastore', 'ldc2_w', 'dstore', 'ldc2_w', 'dstore']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "TRAIN: 0.9815049253280161\n",
      "Epoch 7, step 21 ,acc: 0.964565\n",
      "0.9645650310692093\n",
      "['dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'dmul', 'dload_3', 'dload', 'dmul', 'dsub', 'dstore', 'dload_3', 'dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'iflt', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'ifge', 'aload_1', 'arraylength', 'iconst_1', 'isub', 'istore', 'aload_1', 'iload', 'aaload', 'invokevirtual', 'aload_1', 'iconst_0', 'aaload', 'invokevirtual', 'dsub', 'dstore', 'dload', 'dconst_0', 'dcmpl', 'ifne', 'new', 'dup', 'invokespecial', 'athrow', 'aload_2', 'iconst_1', 'ldc2_w', 'dload', 'ddiv', 'dastore', 'ldc2_w', 'dstore', 'ldc2_w', 'dstore']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "TRAIN: 0.9823555711281597\n",
      "Epoch 8, step 21 ,acc: 0.965288\n",
      "0.9652881937004499\n",
      "['dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'dmul', 'dload_3', 'dload', 'dmul', 'dsub', 'dstore', 'dload_3', 'dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'iflt', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'ifge', 'aload_1', 'arraylength', 'iconst_1', 'isub', 'istore', 'aload_1', 'iload', 'aaload', 'invokevirtual', 'aload_1', 'iconst_0', 'aaload', 'invokevirtual', 'dsub', 'dstore', 'dload', 'dconst_0', 'dcmpl', 'ifne', 'new', 'dup', 'invokespecial', 'athrow', 'aload_2', 'iconst_1', 'ldc2_w', 'dload', 'ddiv', 'dastore', 'ldc2_w', 'dstore', 'ldc2_w', 'dstore']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "TRAIN: 0.9830582729818788\n",
      "Epoch 9, step 21 ,acc: 0.965958\n",
      "0.9659577887293764\n",
      "['dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'dmul', 'dload_3', 'dload', 'dmul', 'dsub', 'dstore', 'dload_3', 'dload', 'dmul', 'dload', 'dload', 'dmul', 'dsub', 'dstore', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'iflt', 'dload', 'dload', 'ddiv', 'dconst_0', 'dcmpg', 'ifge', 'aload_1', 'arraylength', 'iconst_1', 'isub', 'istore', 'aload_1', 'iload', 'aaload', 'invokevirtual', 'aload_1', 'iconst_0', 'aaload', 'invokevirtual', 'dsub', 'dstore', 'dload', 'dconst_0', 'dcmpl', 'ifne', 'new', 'dup', 'invokespecial', 'athrow', 'aload_2', 'iconst_1', 'ldc2_w', 'dload', 'ddiv', 'dastore', 'ldc2_w', 'dstore', 'ldc2_w', 'dstore']\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6f87e6f735e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mfilled_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msequence_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmask_val\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0mlogits_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc_fixed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m             \u001b[0;31m#acc_fixed = session.run([accuracy], feed_dict={seq_len:sequence_len})\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m             \u001b[0;31m#session.run(loc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "N = int(0.9*X_train.shape[0])\n",
    "print(N)\n",
    "testN = X_train.shape[0]-N\n",
    "\n",
    "assert N+testN == X_train.shape[0]\n",
    "n_steps = N//batch_size\n",
    "\n",
    "new_Y = []\n",
    "ln, acsum = 0,0\n",
    "for y_seq in Y_train:\n",
    "    y_seq = list(map(lambda x: ((x[0],1) if x[1]==1 else (x[0],0)), y_seq))\n",
    "    new_Y.append(y_seq)\n",
    "    acsum += sum(list(map((lambda x:x[1]), y_seq)))\n",
    "    ln += len(y_seq)\n",
    "Y_train = new_Y\n",
    "print('BASELINE:', 1-acsum/ln)\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "from random import shuffle\n",
    "indices = np.arange(X_train.shape[0])\n",
    "shuffle(indices)\n",
    "\n",
    "X_train, X_test = X_train[indices[:N]], X_train[indices[N:N+testN]]\n",
    "Y_train, Y_test = Y_train[indices[:N]], Y_train[indices[N:N+testN]]\n",
    "\n",
    "n_steps_test = int(testN/batch_size)\n",
    "\"\"\"\n",
    "X_train, Y_train = data.Data.flattenData(part_1.X, part_1.Y, sequence_length)\n",
    "X_test, Y_test = data.Data.flattenData(part_2.X, part_2.Y, sequence_length)\n",
    "\n",
    "N=len(X_train)\n",
    "print(len(X_test))\n",
    "print(N)\n",
    "print(part_1.baseline)\n",
    "print(part_2.baseline)\n",
    "n_steps = int(len(X_train)/batch_size)\n",
    "n_steps_test = int(len(X_test)/batch_size)\n",
    "with tf.Session(graph=rnn2_graph) as session:\n",
    "    iop = tf.global_variables_initializer()\n",
    "    loc = tf.local_variables_initializer()\n",
    "    session.run(iop)\n",
    "    #session.run(loc)\n",
    "    for epoch in range(num_epochs):\n",
    "        acc_sum=0\n",
    "        batch_index = 0\n",
    "        train_acc = 0\n",
    "        div_sum = 0\n",
    "        for step in range(n_steps):\n",
    "            cur_batch_size = 0\n",
    "            batch = []\n",
    "            batch_Y = []\n",
    "            sequence_len = []\n",
    "            \n",
    "            while(cur_batch_size < batch_size):\n",
    "                cur_seq = []\n",
    "                y_cur_seq = []\n",
    "                for x,y in zip(X_train[batch_index],Y_train[batch_index]):\n",
    "                    if x in ByteCode.mnemonicMap:\n",
    "                        cur_seq.append(x)\n",
    "                        y_cur_seq.append(y)\n",
    "                    if len(cur_seq) == sequence_length:\n",
    "                        batch.append(cur_seq)\n",
    "                        batch_Y.append(y_cur_seq)\n",
    "                        sequence_len.append(len(y_cur_seq))\n",
    "                        cur_batch_size += 1\n",
    "                        cur_seq = []\n",
    "                        y_cur_seq = []\n",
    "                if cur_seq:\n",
    "                    batch.append(cur_seq)\n",
    "                    batch_Y.append(y_cur_seq)\n",
    "                    sequence_len.append(len(y_cur_seq))\n",
    "                    cur_batch_size += 1\n",
    "                batch_index += 1\n",
    "            \n",
    "            data = np.zeros([batch_size, sequence_length])\n",
    "            filled_labels = np.zeros([batch_size, sequence_length, output_size])\n",
    "            filled_labels[:,:,0] = 0\n",
    "            mask_val = np.zeros([batch_size, sequence_length, output_size])\n",
    "            for i, seq in enumerate(batch_Y):\n",
    "                for j, label in enumerate(seq):\n",
    "                    onehot=np.zeros(output_size)\n",
    "                    onehot[label[1]] = 1\n",
    "                    filled_labels[i,j,:] = onehot\n",
    "                    mask_val[i,j,:] = 1\n",
    "            for i in range(batch_size):\n",
    "                for j in range(sequence_len[i]):\n",
    "                    data[i,j] = dictionary.get(batch[i][j],0)+1\n",
    "            \n",
    "            feed = {sequence:data, labels:filled_labels, seq_len:sequence_len, mask:mask_val}\n",
    "            ops = [logits, diff, accuracy, train_step]\n",
    "            logits_val, diff_val, acc_fixed, _= session.run(ops, feed_dict=feed)\n",
    "            #acc_fixed = session.run([accuracy], feed_dict={seq_len:sequence_len})\n",
    "            #session.run(loc)\n",
    "            train_acc += acc_fixed*np.sum(sequence_len)\n",
    "            div_sum += np.sum(sequence_len)\n",
    "            \n",
    "            #print(acc_fixed)\n",
    "            #print(diff_val)\n",
    "            #print(np.sum(filled_labels))\n",
    "            #print(np.sum(sequence_len))\n",
    "            #print(np.argmax(filled_labels,axis=2)[0][:sequence_len[0]])\n",
    "            #print(np.argmax(logits_val,axis=2)[0][:sequence_len[0]])\n",
    "            '''\n",
    "            zum = 0\n",
    "            for i,sl in enumerate(sequence_len):\n",
    "                zum += np.sum(np.argmax(logits_val[i,sl:,:],axis=1))\n",
    "            print(zum)\n",
    "            '''\n",
    "        \n",
    "        print('TRAIN:',train_acc/div_sum)\n",
    "        \n",
    "\n",
    "        acc_sum=0\n",
    "        num=0\n",
    "        acc_pom=0\n",
    "        acc_num = 0\n",
    "            \n",
    "        batch_index = 0\n",
    "        for step in range(n_steps_test):\n",
    "            cur_batch_size = 0\n",
    "            batch = []\n",
    "            batch_Y = []\n",
    "            sequence_len = []\n",
    "            while(cur_batch_size < batch_size):\n",
    "                cur_seq = []\n",
    "                y_cur_seq = []\n",
    "                for x,y in zip(X_test[batch_index],Y_test[batch_index]):\n",
    "                    if x in ByteCode.mnemonicMap:\n",
    "                        cur_seq.append(x)\n",
    "                        y_cur_seq.append(y)\n",
    "                    if len(cur_seq) == sequence_length:\n",
    "                        batch.append(cur_seq)\n",
    "                        batch_Y.append(y_cur_seq)\n",
    "                        sequence_len.append(len(y_cur_seq))\n",
    "                        cur_batch_size += 1\n",
    "                        cur_seq = []\n",
    "                        y_cur_seq = []\n",
    "                if cur_seq:\n",
    "                    batch.append(cur_seq)\n",
    "                    batch_Y.append(y_cur_seq)\n",
    "                    sequence_len.append(len(y_cur_seq))\n",
    "                    cur_batch_size += 1\n",
    "                batch_index += 1\n",
    "                \n",
    "            data = np.zeros([batch_size, sequence_length])\n",
    "            filled_labels = np.zeros([batch_size, sequence_length, output_size])\n",
    "            filled_labels[:,:,0] = 0\n",
    "            mask_val = np.zeros([batch_size, sequence_length, output_size])\n",
    "            for i, seq in enumerate(batch_Y):\n",
    "                for j, label in enumerate(seq):\n",
    "                    onehot=np.zeros(output_size)\n",
    "                    onehot[label[1]] = 1\n",
    "                    filled_labels[i,j,:] = onehot\n",
    "                    mask_val[i,j,:] = 1\n",
    "            for i in range(batch_size):\n",
    "                for j in range(sequence_len[i]):\n",
    "                    data[i,j] = dictionary.get(batch[i][j],0)+1\n",
    "            \n",
    "            feed = {sequence:data, labels:filled_labels, seq_len:sequence_len, mask:mask_val}\n",
    "            ops = [logits, diff, accuracy]\n",
    "            logits_val, diff_val, acc_fixed = session.run(ops, feed_dict=feed)\n",
    "            acc_sum += acc_fixed*np.sum(sequence_len)\n",
    "            acc_num += np.sum(sequence_len)\n",
    "            for logit_gold, logit_pred, sl in zip(filled_labels, logits_val, sequence_len):\n",
    "                acc_pom += np.sum((logit_gold[:sl,1].astype(np.int32)==np.argmax(logit_pred,axis=1)[:sl]))\n",
    "                num += sl\n",
    "                #print(logit_gold[:sl,1].astype(np.int32))\n",
    "                #print(np.argmax(logit_pred,axis=1)[:sl])\n",
    "                #print(acc_pom/num)\n",
    "                \n",
    "            \n",
    "        print(\"Epoch %d, step %d ,acc: %f\" % (epoch, step, acc_sum/acc_num))\n",
    "        print(acc_pom/num)\n",
    "        index = np.argmax(np.sum(np.argmax(filled_labels, axis=2),axis=1))\n",
    "        print(batch[index])\n",
    "        print(np.argmax(filled_labels,axis=2)[index][:sequence_len[index]])\n",
    "        print(np.argmax(logits_val,axis=2)[index][:sequence_len[index]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
