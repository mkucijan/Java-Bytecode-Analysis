{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "import functools\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load embendings\n",
    "\n",
    "embeddings = np.loadtxt(os.path.join(dir_path,'cache','embeddings.vec'))\n",
    "\n",
    "pickle_in = open('cache/database.dict', 'rb')\n",
    "db = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open('cache/traindata.list', 'rb')\n",
    "traindata = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "\n",
    "pickle_in = open('cache/data2onehot.dict', 'rb')\n",
    "dictionary = pickle.load(pickle_in)\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from JavaClassParser import ByteCode\n",
    "reverse_dictionary = dict(zip(dictionary.values(),dictionary.keys()))\n",
    "X_train=[]\n",
    "Y_train=[]\n",
    "X_train_long=[]\n",
    "Y_train_long=[]\n",
    "X_labels = []\n",
    "Y_labels = []\n",
    "\n",
    "for dclass in db.values():\n",
    "    for method in dclass.values():\n",
    "        instructions = method['x']\n",
    "        labels = method['y']\n",
    "        byteIndex = method['index']\n",
    "        \n",
    "        \n",
    "        #\n",
    "        #seperating by labels\n",
    "\n",
    "        cur_section = []\n",
    "        cur_label = labels[0]\n",
    "        if len(instructions)<100:\n",
    "            X_train.append(instructions)\n",
    "            Y_train.append(labels)\n",
    "        else:\n",
    "            X_train_long.append(instructions)\n",
    "            Y_train_long.append(labels)\n",
    "        for instruction, label in zip(instructions,labels):\n",
    "            if label != cur_label:\n",
    "                X_labels.append(cur_section)\n",
    "                Y_labels.append(cur_label)\n",
    "                cur_section = []\n",
    "                cur_label = label\n",
    "            cur_section.append(dictionary.get(instruction,0))\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "Y_train = np.array(Y_train)\n",
    "X_train_long = np.array(X_train_long)\n",
    "Y_train_long = np.array(Y_train_long)\n",
    "X_labels = np.array(X_labels)\n",
    "Y_labels = np.array(Y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv:\n",
    "    def __init__(self, \n",
    "                 learning_rate=1e-1, \n",
    "                 num_epochs=10,\n",
    "                 weight_decay = 1e-2,\n",
    "                 conv1sz = 16,\n",
    "                 conv2sz = 32,\n",
    "                 fc3sz = 512,\n",
    "                 num_classes = 2):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weight_decay = 1e-2\n",
    "        self.conv1sz = conv1sz = 16\n",
    "        self.conv2sz = conv2sz = 32\n",
    "        self.fc3sz = fc3sz = \n",
    "        self.num_classes = num_classes \n",
    "        \n",
    "\n",
    "def cnn_model(features, labels, mode):\n",
    "      weight_decay = self.weight_decay\n",
    "      conv1sz = self.conv1sz\n",
    "      conv2sz = self.conv2sz\n",
    "      fc3sz = self.fc3sz\n",
    "      num_classes = self.num_classes\n",
    "      inputs = tf.reshape(features[\"x\"], [-1, 28, 28, 1])\n",
    "    with tf.contrib.framework.arg_scope([layers.convolution2d],\n",
    "          kernel_size=5, stride=1, padding='SAME', activation_fn=tf.nn.relu,\n",
    "          weights_initializer=layers.variance_scaling_initializer(),\n",
    "          weights_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "\n",
    "        net = layers.convolution2d(inputs, conv1sz, scope='conv1')\n",
    "        net = layers.max_pool2d(net, 2, 2, scope='pool1')\n",
    "        net = layers.convolution2d(net, conv2sz, scope='conv2')\n",
    "        net = layers.max_pool2d(net, 2, 2, scope='pool2')\n",
    "        inputs  = net\n",
    "\n",
    "      with tf.contrib.framework.arg_scope([layers.fully_connected],\n",
    "          activation_fn=tf.nn.relu,\n",
    "          weights_initializer=layers.variance_scaling_initializer(),\n",
    "          weights_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "\n",
    "        net = layers.flatten(inputs)\n",
    "        net = layers.fully_connected(net, fc3sz, scope='fc3')\n",
    "\n",
    "      logits = layers.fully_connected(net, num_classes, activation_fn=None, scope='logits')\n",
    "      #loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(lables=labels, logits=logits))\n",
    "\n",
    "      predictions = {\n",
    "          # Generate predictions (for PREDICT and EVAL mode)\n",
    "          \"classes\": tf.argmax(input=logits, axis=1),\n",
    "          # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "          # `logging_hook`.\n",
    "          \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "      }\n",
    "\n",
    "      if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "      # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "      onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "      loss = tf.losses.softmax_cross_entropy(\n",
    "          onehot_labels=onehot_labels, logits=logits)\n",
    "\n",
    "      # Configure the Training Op (for TRAIN mode)\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "      # Add evaluation metrics (for EVAL mode)\n",
    "      eval_metric_ops = {\n",
    "          \"accuracy\": tf.metrics.accuracy(\n",
    "              labels=labels, predictions=predictions[\"classes\"])}\n",
    "      return tf.estimator.EstimatorSpec(\n",
    "          mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def doublewrap(function):\n",
    "    \"\"\"\n",
    "    A decorator decorator, allowing to use the decorator to be used without\n",
    "    parentheses if not arguments are provided. All arguments must be optional.\n",
    "    \"\"\"\n",
    "    @functools.wraps(function)\n",
    "    def decorator(*args, **kwargs):\n",
    "        if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n",
    "            return function(args[0])\n",
    "        else:\n",
    "            return lambda wrapee: function(wrapee, *args, **kwargs)\n",
    "    return decorator\n",
    "\n",
    "\n",
    "@doublewrap\n",
    "def define_scope(function, scope=None, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    A decorator for functions that define TensorFlow operations. The wrapped\n",
    "    function will only be executed once. Subsequent calls to it will directly\n",
    "    return the result so that operations are added to the graph only once.\n",
    "    The operations added by the function live within a tf.variable_scope(). If\n",
    "    this decorator is used with arguments, they will be forwarded to the\n",
    "    variable scope. The scope name defaults to the name of the wrapped\n",
    "    function.\n",
    "    \"\"\"\n",
    "    attribute = '_cache_' + function.__name__\n",
    "    name = scope or function.__name__\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            with tf.variable_scope(name, *args, **kwargs):\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "    return decorator\n",
    "\n",
    "\n",
    "class Model:\n",
    "\n",
    "    \n",
    "    def __init__(self, image, label,\n",
    "                 learning_rate=1e-1, \n",
    "                 num_epochs=10,\n",
    "                 weight_decay = 1e-2,\n",
    "                 conv1sz = 16,\n",
    "                 conv2sz = 32,\n",
    "                 fc3sz = 512):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_epochs = num_epochs\n",
    "        self.weight_decay\n",
    "        self.conv1sz = conv1sz\n",
    "        self.conv2sz = conv2sz\n",
    "        self.fc3sz = fc3sz\n",
    "        \n",
    "        self.image = image\n",
    "        self.label = label\n",
    "        \n",
    "        self.num_instructions = tf.shape(label)[-1]\n",
    "        \n",
    "        self.prediction\n",
    "        self.optimize\n",
    "        self.error\n",
    "\n",
    "    @define_scope\n",
    "    def prediction(self):\n",
    "        inputs = self.image\n",
    "        with tf.contrib.framework.arg_scope([layers.convolution2d],\n",
    "          kernel_size=5, stride=1, padding='SAME', activation_fn=tf.nn.relu,\n",
    "          weights_initializer=layers.variance_scaling_initializer(),\n",
    "          weights_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "\n",
    "            net = layers.convolution2d(inputs, conv1sz, scope='conv1')\n",
    "            net = layers.max_pool2d(net, 2, 2, scope='pool1')\n",
    "            net = layers.convolution2d(net, conv2sz, scope='conv2')\n",
    "            net = layers.max_pool2d(net, 2, 2, scope='pool2')\n",
    "            inputs  = net\n",
    "            \n",
    "        with tf.contrib.framework.arg_scope([layers.fully_connected],\n",
    "          activation_fn=tf.nn.relu,\n",
    "          weights_initializer=layers.variance_scaling_initializer(),\n",
    "          weights_regularizer=layers.l2_regularizer(weight_decay)):\n",
    "\n",
    "            net = layers.flatten(inputs)\n",
    "            net = layers.fully_connected(net, fc3sz, scope='fc3')\n",
    "        \n",
    "        instr_predict = layers.fully_connected(net, self.num_instructions, activation_fn=None, scope='instructions')\n",
    "        predict = tf.nn.sigmoid(instr_predict, name='predict')\n",
    "        return predict\n",
    "\n",
    "    @define_scope\n",
    "    def optimize(self):\n",
    "        logprob = tf.log(self.prediction + 1e-12)\n",
    "        cross_entropy = -tf.reduce_sum(self.label * logprob)\n",
    "        optimizer = tf.train.RMSPropOptimizer(0.03)\n",
    "        return optimizer.minimize(cross_entropy)\n",
    "\n",
    "    @define_scope\n",
    "    def error(self):\n",
    "        mistakes = tf.not_equal(\n",
    "            tf.argmax(self.label, 1), tf.argmax(self.prediction, 1))\n",
    "        return tf.reduce_mean(tf.cast(mistakes, tf.float32))\n",
    "\n",
    "\n",
    "def main():\n",
    "    mnist = input_data.read_data_sets('./mnist/', one_hot=True)\n",
    "    image = tf.placeholder(tf.float32, [None, 784])\n",
    "    label = tf.placeholder(tf.float32, [None, 10])\n",
    "    model = Model(image, label)\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "\n",
    "    for _ in range(10):\n",
    "      images, labels = mnist.test.images, mnist.test.labels\n",
    "      error = sess.run(model.error, {image: images, label: labels})\n",
    "      print('Test error {:6.2f}%'.format(100 * error))\n",
    "      for _ in range(60):\n",
    "        images, labels = mnist.train.next_batch(100)\n",
    "sess.run(model.optimize, {image: images, label: labels})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
